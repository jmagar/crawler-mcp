### Unit Tests Structure
```
crawler_mcp/crawlers/optimized/tests/
├── test_crawling_tools.py        # NEW: Tool integration tests
├── test_crawling_models.py       # NEW: Model validation tests
├── test_crawling_validation.py   # NEW: Parameter validation tests
└── fixtures/                     # Test data and mocks
    ├── sample_website/
    ├── sample_repository/
    └── sample_directory/
```

### Test Categories

**1. Parameter Validation Tests**
```python
async def test_website_crawl_parameter_validation():
    """Test URL validation, limits, and pattern validation"""
    # Invalid URL
    with pytest.raises(ToolError, match="Invalid URL"):
        await crawl_website(ctx, "not-a-url")
    
    # Limits exceeded  
    with pytest.raises(ToolError, match="max_urls exceeds limit"):
        await crawl_website(ctx, "https://example.com", max_urls=50000)
    
    # Invalid regex pattern
    with pytest.raises(ToolError, match="Invalid regex"):
        await crawl_website(ctx, "https://example.com", 
                          include_patterns=["[invalid"])
```

**2. Integration Tests**
```python
async def test_website_crawl_with_optimized_strategy():
    """Test full website crawl integration"""
    response = await crawl_website(
        ctx, "https://httpbin.org", 
        max_urls=5, max_concurrent=2
    )
    
    assert response.success is True
    assert response.crawl_type == "website"
    assert response.pages_crawled > 0
    assert len(response.sample_pages) <= 3
    assert response.output_paths["ndjson"].exists()
```

**3. Output Validation Tests**
```python
async def test_output_manager_integration():
    """Test OutputManager integration and file structure"""
    response = await crawl_website(ctx, "https://example.com")
    
    # Verify output structure
    domain_dir = Path(output_dir) / "crawls" / "example_com" / "latest"
    assert (domain_dir / "combined.html").exists()
    assert (domain_dir / "pages.ndjson").exists()
    assert (domain_dir / "report.json").exists()
    
    # Verify backup rotation
    response2 = await crawl_website(ctx, "https://example.com")
    backup_dir = domain_dir.parent / "backup"
    assert backup_dir.exists()
```

**4. Performance & Memory Tests**
```python
async def test_large_crawl_memory_management():
    """Test memory handling with larger crawls"""
    # Mock a site with many URLs
    response = await crawl_website(
        ctx, "https://mock-large-site.com", 
        max_urls=1000, max_concurrent=16
    )
    
    # Verify memory monitoring kicked in
    assert "memory_pressure_handling" in response.config_used
    assert response.pages_crawled > 0  # Some pages completed
```

### Validation Matrix

| Test Scenario | Website | Repository | Directory | Status |
|---------------|---------|------------|-----------|---------|
| **Basic Functionality** |
| Valid single URL/path | ✅ | ✅ | ✅ | Required |
| Invalid URL/path | ✅ | ✅ | ✅ | Required |
| Empty results handling | ✅ | ✅ | ✅ | Required |
| **Parameter Limits** |
| Max URLs/files exceeded | ✅ | ✅ | ✅ | Required |
| Max concurrency exceeded | ✅ | N/A | N/A | Required |
| Max depth exceeded | N/A | N/A | ✅ | Required |
| **Pattern Matching** |
| Include patterns | ✅ | ✅ | ✅ | Required |
| Exclude patterns | ✅ | ✅ | ✅ | Required |
| File extensions | N/A | ✅ | ✅ | Required |
| **Integration** |
| OutputManager integration | ✅ | ✅ | ✅ | Required |
| Progress reporting | ✅ | ✅ | ✅ | Required |
| Error handling | ✅ | ✅ | ✅ | Required |
| **Optional Features** |
| Embeddings generation | ✅ | ✅ | ✅ | Optional |
| RAG ingestion | ✅ | ✅ | ✅ | Optional |
| Multiple output formats | ✅ | ✅ | ✅ | Nice-to-have |

## Phase 5: Rollout & Cutover Plan

### Implementation Phases

**Phase 5.1: Core Implementation (Week 1)**
1. Create `crawling_models.py` with request/response models
2. Implement `CrawlingToolsManager` base class
3. Implement `crawl_website` tool with basic functionality
4. Add server integration and registration

**Phase 5.2: Extended Tools (Week 2)**  
1. Implement `crawl_repository` tool with Git integration
2. Implement `crawl_directory` tool with filesystem traversal
3. Add comprehensive parameter validation
4. Integrate progress reporting throughout

**Phase 5.3: Integration & Testing (Week 3)**
1. Complete unit test suite implementation
2. Integration testing with existing optimized components
3. Performance testing and memory validation
4. Documentation and example creation

**Phase 5.4: Advanced Features (Week 4)**
1. RAG ingestion integration (optional flag)
2. Multiple output format support  
3. Enhanced error handling and recovery
4. Final validation and deployment preparation

### Risk Mitigation

**High Risk: Memory Management**
- *Mitigation*: Leverage existing memory monitoring in OptimizedCrawlerStrategy
- *Fallback*: Automatic concurrency reduction on memory pressure

**Medium Risk: Parameter Validation Complexity**
- *Mitigation*: Use Pydantic models for validation with existing CLI patterns
- *Fallback*: Start with basic validation, enhance iteratively

**Low Risk: Output Format Compatibility**
- *Mitigation*: Use existing OutputManager patterns
- *Fallback*: Focus on structured format first, add others later

### Success Criteria

**Functional Requirements:**
- ✅ All three tools (`crawl_website`, `crawl_repository`, `crawl_directory`) operational
- ✅ Parameter validation prevents invalid requests
- ✅ Structured outputs saved to `output/crawls/` directory structure
- ✅ Progress reporting visible in MCP client
- ✅ Integration with existing optimized architecture

**Performance Requirements:**
- ✅ Website crawling: >10 pages/second on typical sites
- ✅ Memory usage: <2GB for 1000-page crawls  
- ✅ Concurrent safety: No conflicts with existing tools

**Quality Requirements:**
- ✅ >90% test coverage for new tool modules
- ✅ Error handling covers network failures, invalid content, memory pressure
- ✅ Documentation includes API examples and troubleshooting

This implementation plan leverages the mature, high-performance Optimized Crawler architecture while providing clean MCP tool interfaces that integrate seamlessly with the existing server infrastructure.
