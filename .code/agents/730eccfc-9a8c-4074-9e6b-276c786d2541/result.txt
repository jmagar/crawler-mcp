Based on my analysis of the optimized crawler architecture, here's a concrete implementation plan for the two MCP tools:

# Implementation Plan for Optimized Crawling Tools

## File Layout

```
crawler_mcp/crawlers/optimized/tools/
├── crawling_tools.py          # New file - contains both tools
└── __init__.py               # Updated to export register_crawling_tools
```

## API Signatures & Parameters

### Tool 1: `scrape` - Single Page Intelligence

```python
@mcp.tool
async def scrape(
    ctx: Context,
    url: str,
    screenshot: bool = False,
    wait_for: str | None = None,
    css_selector: str | None = None,
    extract_images: bool = False,
    javascript: bool | None = None,
    timeout_ms: int = 30000,
    rag_ingest: bool = False
) -> dict[str, Any]
```

**Parameter Validation:**
- `url`: Required, validate URL format
- `timeout_ms`: Clamp between 5000-120000ms
- `wait_for`: CSS selector or JS condition string
- `css_selector`: Valid CSS selector for focused extraction
- `javascript`: Override config if specified, default to config.javascript_enabled

### Tool 2: `crawl` - Unified Smart Crawling

```python
@mcp.tool
async def crawl(
    ctx: Context,
    target: str,
    max_pages: int = 100,
    max_concurrent: int | None = None,
    depth: int = 3,
    include_patterns: list[str] | None = None,
    exclude_patterns: list[str] | None = None,
    screenshot_samples: int = 0,
    javascript: bool | None = None,
    timeout_ms: int = 30000,
    rag_ingest: bool = False
) -> dict[str, Any]
```

**Parameter Validation & Clamping:**
- `max_pages`: Clamp between 1-10000, default 100
- `max_concurrent`: If None, use config.max_concurrent_crawls, else clamp 1-50
- `depth`: Clamp between 1-10 for websites/directories
- `screenshot_samples`: Clamp 0-10, random sampling if >0
- `timeout_ms`: Clamp 5000-120000ms

## Input Type Detection Logic

```python
def detect_target_type(target: str) -> tuple[str, dict[str, Any]]:
    """Returns (type, parsed_info)"""
    if re.match(r'^https?://', target):
        return 'website', {'url': target}
    elif re.match(r'^https://github\.com/[^/]+/[^/]+/pull/\d+', target):
        return 'github_pr', parse_pr_url(target)
    elif re.match(r'^https://github\.com/[^/]+/[^/]+', target):
        return 'github_repo', parse_repo_url(target)
    elif os.path.isdir(target):
        return 'directory', {'path': Path(target).absolute()}
    elif target.endswith('.git') or 'github.com' in target:
        return 'git_repo', {'url': target}
    else:
        raise ToolError(f"Cannot detect target type for: {target}")
```

## Result Schemas

### Scrape Result Schema
```python
{
    "success": bool,
    "url": str,
    "title": str | None,
    "content_preview": str,  # First 500 chars
    "word_count": int,
    "language": str | None,
    "links_found": int,
    "images_found": int,
    "loading_time_ms": int,
    "metadata": {
        "extraction_method": str,
        "has_javascript": bool,
        "response_code": int,
        "content_type": str | None,
    },
    "outputs": {
        "content_path": str | None,
        "screenshot_path": str | None,
        "html_path": str | None,
    }
}
```

### Crawl Result Schema
```python
{
    "success": bool,
    "kind": str,  # website|directory|github_pr|github_repo
    "target": str,
    "pages_crawled": int,
    "pages_failed": int,
    "duration_seconds": float,
    "pages_per_second": float,
    "content_preview": str,  # Combined first 1000 chars
    "statistics": {
        "total_size_mb": float,
        "avg_page_size_bytes": int,
        "unique_domains": int,
        "languages_detected": list[str],
    },
    "outputs": {
        "manifest_path": str,
        "content_dir": str,
        "report_path": str,
        "screenshot_paths": list[str],
    },
    "rag_ingestion": {
        "enabled": bool,
        "pages_ingested": int,
        "vector_count": int,
    } | None
}
```

## Progress Phase Mapping

### Scrape Phases (3 phases)
1. **"Initializing"** (0-20%): Browser setup, URL validation
2. **"Fetching"** (20-80%): Page load, content extraction, optional screenshot
3. **"Processing"** (80-100%): Content analysis, output saving, RAG ingestion

### Crawl Phases (5 phases)
1. **"Planning"** (0-10%): Target detection, URL/path discovery
2. **"Preparing"** (10-20%): Browser setup, strategy selection
3. **"Crawling"** (20-80%): Parallel crawling with sub-progress
4. **"Processing"** (80-95%): Content analysis, deduplication, conversion
5. **"Finalizing"** (95-100%): Output saving, RAG ingestion, cleanup

## Import Map & Core Integration

### Core Components Used
```python
# From optimized core
from ..config import OptimizedConfig
from ..core.strategy import OptimizedCrawlerStrategy
from ..core.parallel_engine import ParallelEngine
from ..factories.browser_factory import BrowserFactory
from ..utils.output_manager import OutputManager
from ..utils.log_manager import LogManager
from ..clients.github_client import GitHubClient

# From models
from ....models.crawl import PageContent, CrawlResult

# For GitHub PR detection/processing
from ..tools.github_pr_tools import list_pr_items_impl
```

### OutputManager Integration
- Use `output_manager.get_crawl_output_paths()` for file paths
- Save content via `output_manager.save_crawl_outputs()`
- Respect `output_manager.sanitize_domain()` for naming

### Progress Reporting Pattern
```python
await ctx.report_progress(
    progress=current_phase_progress,
    total=100,
    description=f"{phase_name}: {current_action}"
)
```

## Error Handling Approach

### Validation Errors
```python
# Parameter validation
if not url or not url.startswith(('http://', 'https://')):
    raise ToolError("Invalid URL format")

# Timeout clamping
timeout_ms = max(5000, min(120000, timeout_ms))
```

### Runtime Errors
```python
try:
    result = await strategy.crawl(url, **params)
except Exception as e:
    return {
        "success": False,
        "error": f"Crawl failed: {str(e)}",
        "error_type": type(e).__name__,
        "outputs": {}
    }
```

## Implementation Steps

### Phase 1: Core Infrastructure (2-3 hours)
1. Create `crawling_tools.py` with function stubs
2. Implement input type detection logic
3. Wire into `server.py` with registration
4. Add basic parameter validation and error handling

### Phase 2: Scrape Tool (3-4 hours)
1. Implement single-page scraping logic using OptimizedCrawlerStrategy
2. Add screenshot capture via browser factories
3. Implement progress reporting for 3 phases
4. Add output saving via OutputManager

### Phase 3: Crawl Tool (4-6 hours)
1. Implement website crawling path (URL strategy)
2. Implement directory crawling path (file scanning)
3. Implement GitHub PR path (using existing github_pr_tools)
4. Add unified result schema and progress reporting

### Phase 4: RAG Integration (2-3 hours)
1. Add optional RAG ingestion logic for both tools
2. Test embedding generation and Qdrant storage
3. Validate against existing RAG pipeline

### Phase 5: Testing & Polish (2-3 hours)
1. Unit tests for input detection and parameter validation
2. Integration tests with sample URLs/directories/PRs
3. Error handling edge cases
4. Performance validation

## Test Plan

### Unit Tests
- Input type detection accuracy
- Parameter validation and clamping
- Error message formatting
- Progress calculation logic

### Integration Tests
- Single page scraping (HTML, JS-heavy, API endpoints)
- Website crawling (small sites, 10-50 pages)
- Directory crawling (code repos, docs folders)
- GitHub PR processing (public PRs)
- RAG ingestion pipeline (if enabled)

### Performance Tests
- Memory usage under concurrent load
- Response time for typical operations
- Progress reporting accuracy
- Output file size validation

## Cutover Checklist

### Pre-deployment
- [ ] All unit and integration tests passing
- [ ] Error handling covers edge cases
- [ ] Progress reporting works correctly
- [ ] Output paths follow existing conventions
- [ ] RAG ingestion optional and working
- [ ] Documentation updated in docstrings

### Deployment
- [ ] Register tools in `server.py`
- [ ] Verify both tools appear in MCP client
- [ ] Test sample operations end-to-end
- [ ] Check output file creation and cleanup
- [ ] Validate performance meets expectations

### Post-deployment
- [ ] Monitor error rates and performance
- [ ] Collect user feedback on UX
- [ ] Plan future enhancements based on usage patterns

This plan provides a systematic approach to implementing both crawling tools while leveraging the existing optimized infrastructure and maintaining consistency with the established patterns.
