[DEBUG] CLI: Delegating hierarchical memory load to server for CWD: /home/jmagar/code/crawl-mcp (memoryImportFormat: tree)
[DEBUG] [MemoryDiscovery] Loading server hierarchical memory for CWD: /home/jmagar/code/crawl-mcp (importFormat: tree)
[DEBUG] [MemoryDiscovery] Searching for GEMINI.md starting from CWD: /home/jmagar/code/crawl-mcp
[DEBUG] [MemoryDiscovery] Determined project root: /home/jmagar/code/crawl-mcp
[DEBUG] [BfsFileSearch] Scanning [1/200]: batch of 1
[DEBUG] [BfsFileSearch] Scanning [14/200]: batch of 13
[DEBUG] [BfsFileSearch] Scanning [29/200]: batch of 15
[DEBUG] [BfsFileSearch] Scanning [44/200]: batch of 15
[DEBUG] [BfsFileSearch] Scanning [59/200]: batch of 15
[DEBUG] [BfsFileSearch] Scanning [74/200]: batch of 15
[DEBUG] [BfsFileSearch] Scanning [89/200]: batch of 15
[DEBUG] [BfsFileSearch] Scanning [104/200]: batch of 15
[DEBUG] [BfsFileSearch] Scanning [119/200]: batch of 15
[DEBUG] [BfsFileSearch] Scanning [134/200]: batch of 15
[DEBUG] [BfsFileSearch] Scanning [149/200]: batch of 15
[DEBUG] [BfsFileSearch] Scanning [164/200]: batch of 15
[DEBUG] [BfsFileSearch] Scanning [179/200]: batch of 15
[DEBUG] [BfsFileSearch] Scanning [194/200]: batch of 15
[DEBUG] [BfsFileSearch] Scanning [200/200]: batch of 6
[DEBUG] [MemoryDiscovery] Final ordered GEMINI.md paths to read: []
[DEBUG] [MemoryDiscovery] No GEMINI.md files found in hierarchy of the workspace.
Flushing log events to Clearcut.
Data collection is disabled.
Of course. Here is a plan to update the `scrape` tool.

### Patch Plan

1.  **Modify `crawler_mcp/crawlers/optimized/tools/crawling.py`:**
    *   Introduce a new constant `MAX_CLEAN_LEN = 50000` to control the maximum length of the returned cleaned content.
    *   Create a new private helper function `_get_clean_content(resp: OptimizedCrawlResponse, pages: list[PageContent]) -> str` within the module. This function will implement the required content selection logic:
        1.  Return `resp.extracted_content` if it's available and not placeholder text.
        2.  If not, iterate through the first page's markdown fields (`fit_markdown`, `raw_markdown`, `markdown`) and return the first non-placeholder content found.
        3.  As a fallback, use the first page's `content` field if it's not a placeholder.
        4.  As a final resort, strip HTML tags from the first page's `html` and return the result.
        5.  If no usable content is found, return an empty string.
    *   In the `scrape` tool function, after the crawler strategy has run, call `_get_clean_content` to get the cleaned text.
    *   Add two new keys to the returned dictionary:
        *   `clean_content`: The cleaned content, truncated to `MAX_CLEAN_LEN`.
        *   `clean_truncated`: A boolean set to `True` if the content was longer than `MAX_CLEAN_LEN`, otherwise `False`.

2.  **Update Tests in `crawler_mcp/crawlers/optimized/tests/test_crawling_tools.py`:**
    *   Locate the `test_scrape_happy_path` test.
    *   Update the mock `FakeStrategy` to ensure its `execute` method returns an `OptimizedCrawlResponse` object that includes a sample string for the `extracted_content` attribute.
    *   Add assertions to the test to verify:
        *   The `clean_content` key exists in the result from the `scrape` tool.
        *   The value of `clean_content` matches the mock `extracted_content`.
        *   The `clean_truncated` key exists and is `False`.

This plan directly addresses the requirements by using `resp.extracted_content` as the primary source for cleaned text, implementing the specified fallbacks, and adding the necessary tests for verification, all while avoiding modifications to legacy code paths.
