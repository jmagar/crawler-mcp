Top-level structure (depth 2):
.
.cache
.cache/mypy
.cache/pytest
.cache/ruff
.claude
.code
.code/agents
.git
.git/branches
.git/hooks
.git/info
.git/logs
.git/objects
.git/refs
.venv
.venv/bin
.venv/include
.venv/lib
.venv/share
crawler_mcp
crawler_mcp.egg-info
crawler_mcp/.claude
crawler_mcp/__pycache__
crawler_mcp/core
crawler_mcp/crawlers
crawler_mcp/middleware
crawler_mcp/models
crawler_mcp/tools
crawler_mcp/types
crawler_mcp/webhook
docs
logs
output
output/cache
output/crawls
output/logs
output/pr
scripts
scripts/.claude
scripts/__pycache__
tests
tests/__pycache__
webhook_outputs

Optimized package (depth 2):
crawler_mcp/crawlers/optimized
crawler_mcp/crawlers/optimized/.claude
crawler_mcp/crawlers/optimized/__pycache__
crawler_mcp/crawlers/optimized/cli
crawler_mcp/crawlers/optimized/cli/__pycache__
crawler_mcp/crawlers/optimized/clients
crawler_mcp/crawlers/optimized/clients/__pycache__
crawler_mcp/crawlers/optimized/core
crawler_mcp/crawlers/optimized/core/__pycache__
crawler_mcp/crawlers/optimized/factories
crawler_mcp/crawlers/optimized/factories/__pycache__
crawler_mcp/crawlers/optimized/logs
crawler_mcp/crawlers/optimized/middleware
crawler_mcp/crawlers/optimized/middleware/__pycache__
crawler_mcp/crawlers/optimized/models
crawler_mcp/crawlers/optimized/models/__pycache__
crawler_mcp/crawlers/optimized/output
crawler_mcp/crawlers/optimized/output/cache
crawler_mcp/crawlers/optimized/output/crawls
crawler_mcp/crawlers/optimized/output/logs
crawler_mcp/crawlers/optimized/output/pr
crawler_mcp/crawlers/optimized/processing
crawler_mcp/crawlers/optimized/processing/__pycache__
crawler_mcp/crawlers/optimized/shared
crawler_mcp/crawlers/optimized/shared/__pycache__
crawler_mcp/crawlers/optimized/tests
crawler_mcp/crawlers/optimized/tests/__pycache__
crawler_mcp/crawlers/optimized/tools
crawler_mcp/crawlers/optimized/tools/__pycache__
crawler_mcp/crawlers/optimized/utils
crawler_mcp/crawlers/optimized/utils/__pycache__

Search for crawl-related core/processing modules:
crawler_mcp/crawlers/optimized/AGENTS.md:4:- `crawler_mcp/`: core package (server, config, tools, middleware, services, webhook).
crawler_mcp/crawlers/optimized/AGENTS.md:7:- Entry points: `crawler_mcp/crawlers/optimized/server.py` (MCP server), `crawler_mcp/webhook/server.py` (webhook).
crawler_mcp/crawlers/optimized/AGENTS.md:8:- MCP tools live in `crawler_mcp/crawlers/optimized/tools/` and are registered in `crawler_mcp/crawlers/optimized/server.py`.
crawler_mcp/crawlers/optimized/AGENTS.md:12:- Run server (dev): `fastmcp dev crawler_mcp/crawlers/optimized/server.py`
crawler_mcp/crawlers/optimized/AGENTS.md:13:- Run server (direct): `uv run python -m crawler_mcp.crawlers.optimized.server` or `uv run crawler-mcp`
crawler_mcp/crawlers/optimized/AGENTS.md:14:- Webhook (direct): `uv run python -m crawler_mcp.webhook.server` or `uv run crawler-webhook`
crawler_mcp/crawlers/optimized/AGENTS.md:16:- Full tests + coverage: `uv run pytest --cov=crawler_mcp --cov-report=term-missing`
crawler_mcp/crawlers/optimized/AGENTS.md:18:- Types: `uv run mypy crawler_mcp`
crawler_mcp/crawlers/optimized/AGENTS.md:24:- Type hints required; mypy runs in strict mode inside `crawler_mcp/`.
crawler_mcp/crawlers/optimized/AGENTS.md:43:- Install for Claude Desktop: `fastmcp install claude-desktop crawler_mcp/crawlers/optimized/server.py`.
crawler_mcp/crawlers/optimized/AGENTS.md:44:- Register new MCP tools under `crawler_mcp/crawlers/optimized/tools/` and wire them in `register_*_tools` within the server.
crawler_mcp/crawlers/optimized/config.py:2:Configuration management for optimized high-performance web crawler.
crawler_mcp/crawlers/optimized/config.py:4:This module provides centralized configuration management for the optimized crawler,
crawler_mcp/crawlers/optimized/config.py:15:    """Configuration for optimized crawler with high-performance defaults"""
crawler_mcp/crawlers/optimized/config.py:19:    """Maximum number of URLs to discover from sitemaps before crawling"""
crawler_mcp/crawlers/optimized/config.py:22:    """Minimum relevance score for URLs to be included in crawling"""
crawler_mcp/crawlers/optimized/config.py:50:    max_concurrent_crawls: int = 16
crawler_mcp/crawlers/optimized/config.py:51:    """Maximum number of concurrent crawling sessions (optimal for i7-13700K)"""
crawler_mcp/crawlers/optimized/config.py:82:    """Enable crawl4ai caching for improved performance"""
crawler_mcp/crawlers/optimized/config.py:84:    browser_headless: bool = True
crawler_mcp/crawlers/optimized/config.py:85:    """Run browser in headless mode for better performance"""
crawler_mcp/crawlers/optimized/config.py:88:    """Disable image loading for maximum crawling speed"""
crawler_mcp/crawlers/optimized/config.py:99:    """Ignore robots.txt rules - aggressive crawling as requested"""
crawler_mcp/crawlers/optimized/config.py:104:    respect_crawl_delay: bool = False
crawler_mcp/crawlers/optimized/config.py:105:    """Ignore crawl delay directives for maximum throughput"""
crawler_mcp/crawlers/optimized/config.py:115:    browser_pool_size: int = 4
crawler_mcp/crawlers/optimized/config.py:116:    """Number of browser instances in pool (for non-builtin mode)"""
crawler_mcp/crawlers/optimized/config.py:126:    """Extract links from crawled pages"""
crawler_mcp/crawlers/optimized/config.py:129:    """Extract image URLs from crawled pages"""
crawler_mcp/crawlers/optimized/config.py:154:    """Max concurrent TEI batch requests (post-crawl embedding phase)"""
crawler_mcp/crawlers/optimized/config.py:202:    qdrant_collection: str = "crawler_pages"
crawler_mcp/crawlers/optimized/config.py:226:    """Use JS-enabled browser for fallback expansion to handle dynamic nav/docs sites."""
crawler_mcp/crawlers/optimized/config.py:236:    """Restrict crawling to these locale prefixes (e.g., ['en']). Empty = no filter.
crawler_mcp/crawlers/optimized/config.py:259:    """Base output directory for all crawl results and reports"""
crawler_mcp/crawlers/optimized/config.py:295:            "max_concurrent_crawls": "MAX_CONCURRENT",
crawler_mcp/crawlers/optimized/config.py:301:            "browser_headless": "HEADLESS",
crawler_mcp/crawlers/optimized/config.py:386:        config.respect_crawl_delay = False
crawler_mcp/crawlers/optimized/config.py:430:        aggressive.max_concurrent_crawls = 28
crawler_mcp/crawlers/optimized/config.py:435:        aggressive.browser_pool_size = 8
crawler_mcp/crawlers/optimized/config.py:453:        conservative.max_concurrent_crawls = 8
crawler_mcp/crawlers/optimized/config.py:472:        if self.max_concurrent_crawls < 1:
crawler_mcp/crawlers/optimized/config.py:473:            errors.append("max_concurrent_crawls must be at least 1")
crawler_mcp/crawlers/optimized/config.py:498:            f"OptimizedConfig(concurrent={self.max_concurrent_crawls}, "
crawler_mcp/crawlers/optimized/run.py:3:from .cli.crawl import main
crawler_mcp/crawlers/optimized/run.py:5:"""Thin wrapper to invoke the crawl CLI.
crawler_mcp/crawlers/optimized/run.py:8:`crawler_mcp.crawlers.optimized.run`. The implementation now lives in
crawler_mcp/crawlers/optimized/run.py:9:`crawler_mcp.crawlers.optimized.cli.crawl`.
crawler_mcp/crawlers/optimized/utils/log_manager.py:1:"""Log management for the optimized crawler.
crawler_mcp/crawlers/optimized/utils/log_manager.py:14:    """Manages crawler logging with automatic rotation."""
crawler_mcp/crawlers/optimized/utils/log_manager.py:26:        self.crawl_log_path = self.log_dir / "crawl.log"
crawler_mcp/crawlers/optimized/utils/log_manager.py:44:    def setup_crawl_logger(self, name: str = "crawl") -> logging.Logger:
crawler_mcp/crawlers/optimized/utils/log_manager.py:45:        """Setup main crawl logger with rotation.
crawler_mcp/crawlers/optimized/utils/log_manager.py:60:            filename=self.crawl_log_path,
crawler_mcp/crawlers/optimized/utils/log_manager.py:148:            name: Logger type ('crawl', 'errors', 'console')
crawler_mcp/crawlers/optimized/utils/log_manager.py:153:        if name == "crawl":
crawler_mcp/crawlers/optimized/utils/log_manager.py:154:            return self.setup_crawl_logger()
crawler_mcp/crawlers/optimized/utils/log_manager.py:160:            # Default to crawl logger
crawler_mcp/crawlers/optimized/utils/log_manager.py:161:            return self.setup_crawl_logger(name)
crawler_mcp/crawlers/optimized/utils/log_manager.py:172:            ("crawl", self.crawl_log_path),
crawler_mcp/crawlers/optimized/utils/log_manager.py:197:            ("crawl", self.crawl_log_path),
crawler_mcp/crawlers/optimized/utils/log_manager.py:244:            log_type: "crawl", "errors", or "both"
crawler_mcp/crawlers/optimized/utils/log_manager.py:246:        if log_type in ("crawl", "both"):
crawler_mcp/crawlers/optimized/utils/log_manager.py:247:            logger = self.get_logger("crawl")
crawler_mcp/crawlers/optimized/utils/output_manager.py:1:"""Output management for the optimized crawler.
crawler_mcp/crawlers/optimized/utils/output_manager.py:34:        self.crawls_dir = self.base_dir / "crawls"
crawler_mcp/crawlers/optimized/utils/output_manager.py:40:        self.crawls_index = self.crawls_dir / "_index.json"
crawler_mcp/crawlers/optimized/utils/output_manager.py:48:        for dir_path in [self.crawls_dir, self.pr_dir, self.cache_dir, self.logs_dir]:
crawler_mcp/crawlers/optimized/utils/output_manager.py:82:    def get_crawl_output_paths(self, url: str) -> dict[str, Path]:
crawler_mcp/crawlers/optimized/utils/output_manager.py:83:        """Get all output paths for a crawl.
crawler_mcp/crawlers/optimized/utils/output_manager.py:86:            url: URL being crawled
crawler_mcp/crawlers/optimized/utils/output_manager.py:92:        latest_dir = self.crawls_dir / domain / "latest"
crawler_mcp/crawlers/optimized/utils/output_manager.py:124:    def rotate_crawl_backup(self, domain: str) -> None:
crawler_mcp/crawlers/optimized/utils/output_manager.py:125:        """Move latest/ to backup/ before new crawl.
crawler_mcp/crawlers/optimized/utils/output_manager.py:130:        domain_dir = self.crawls_dir / domain
crawler_mcp/crawlers/optimized/utils/output_manager.py:142:    def save_crawl_outputs(
crawler_mcp/crawlers/optimized/utils/output_manager.py:145:        """Save all crawl outputs with automatic rotation.
crawler_mcp/crawlers/optimized/utils/output_manager.py:153:        paths = self.get_crawl_output_paths(f"https://{domain}")
crawler_mcp/crawlers/optimized/utils/output_manager.py:184:            # Log error but don't fail the crawl
crawler_mcp/crawlers/optimized/utils/output_manager.py:188:        """Update _index.json with crawl metadata.
crawler_mcp/crawlers/optimized/utils/output_manager.py:196:            if self.crawls_index.exists():
crawler_mcp/crawlers/optimized/utils/output_manager.py:197:                with open(self.crawls_index, encoding="utf-8") as f:
crawler_mcp/crawlers/optimized/utils/output_manager.py:221:            with open(self.crawls_index, "w", encoding="utf-8") as f:
crawler_mcp/crawlers/optimized/utils/output_manager.py:229:        domain_dir = self.crawls_dir / domain
crawler_mcp/crawlers/optimized/utils/output_manager.py:270:            if not self.crawls_index.exists():
crawler_mcp/crawlers/optimized/utils/output_manager.py:273:            with open(self.crawls_index, encoding="utf-8") as f:
crawler_mcp/crawlers/optimized/utils/output_manager.py:286:                backup_dir = self.crawls_dir / domain / "backup"
crawler_mcp/crawlers/optimized/utils/output_manager.py:307:                    domain_dir = self.crawls_dir / domain
crawler_mcp/crawlers/optimized/utils/output_manager.py:320:            with open(self.crawls_index, "w", encoding="utf-8") as f:
crawler_mcp/crawlers/optimized/__init__.py:2:Optimized high-performance web crawler package.
crawler_mcp/crawlers/optimized/__init__.py:4:This package provides a modular, high-performance web crawler implementation
crawler_mcp/crawlers/optimized/__init__.py:8:- URL discovery from sitemaps without initial crawling
crawler_mcp/crawlers/optimized/__init__.py:14:- Ignores robots.txt for aggressive crawling (as requested)
crawler_mcp/crawlers/optimized/__init__.py:17:    from crawler_mcp.crawlers.optimized import OptimizedCrawlerStrategy, OptimizedConfig
crawler_mcp/crawlers/optimized/__init__.py:21:        max_concurrent_crawls=16,
crawler_mcp/crawlers/optimized/__init__.py:32:            response = await strategy.crawl("https://example.com")
crawler_mcp/crawlers/optimized/__init__.py:33:            print(f"Crawled {response.metadata['pages_crawled']} pages")
crawler_mcp/crawlers/optimized/__init__.py:41:from .core.parallel_engine import CrawlStats, ParallelEngine
crawler_mcp/crawlers/optimized/__init__.py:43:from .factories.browser_factory import BrowserFactory
crawler_mcp/crawlers/optimized/__init__.py:55:__description__ = "High-performance web crawler with Crawl4AI integration"
crawler_mcp/crawlers/optimized/README.md:3:A sophisticated, production-ready web crawling system built on Crawl4AI with advanced features including GPU-aware TEI embeddings, Qdrant vector storage, adaptive concurrency control, and comprehensive content extraction capabilities.
crawler_mcp/crawlers/optimized/README.md:7:This crawler represents a complete overhaul of traditional web scraping approaches, providing:
crawler_mcp/crawlers/optimized/README.md:22:crawler_mcp/crawlers/optimized/
crawler_mcp/crawlers/optimized/README.md:29:│   ├── crawl.py               # Main crawl CLI with structured output
crawler_mcp/crawlers/optimized/README.md:33:│   ├── strategy.py            # Main crawler strategy (AsyncCrawlerStrategy)
crawler_mcp/crawlers/optimized/README.md:34:│   ├── parallel_engine.py     # High-performance parallel execution
crawler_mcp/crawlers/optimized/README.md:38:│   ├── browser_factory.py     # Browser configuration factory
crawler_mcp/crawlers/optimized/README.md:77:# Core crawler settings
crawler_mcp/crawlers/optimized/README.md:106:Basic crawl with progress monitoring and structured output:
crawler_mcp/crawlers/optimized/README.md:108:uv run python -m crawler_mcp.crawlers.optimized.run \
crawler_mcp/crawlers/optimized/README.md:114:- `output/crawls/{domain}/latest/combined.html` - Full crawl HTML
crawler_mcp/crawlers/optimized/README.md:115:- `output/crawls/{domain}/latest/pages.ndjson` - Individual page data
crawler_mcp/crawlers/optimized/README.md:116:- `output/crawls/{domain}/latest/report.json` - Performance report
crawler_mcp/crawlers/optimized/README.md:117:- `output/logs/crawl.log` - Execution logs (rotated at 10MB)
crawler_mcp/crawlers/optimized/README.md:119:Previous crawl backed up to `output/crawls/{domain}/backup/`
crawler_mcp/crawlers/optimized/README.md:123:The crawler uses a structured output system that automatically organizes results:
crawler_mcp/crawlers/optimized/README.md:127:├── crawls/
crawler_mcp/crawlers/optimized/README.md:129:│   │   ├── latest/            # Most recent crawl
crawler_mcp/crawlers/optimized/README.md:133:│   │   └── backup/            # Previous crawl backup
crawler_mcp/crawlers/optimized/README.md:147:    ├── crawl.log              # Main execution log
crawler_mcp/crawlers/optimized/README.md:148:    ├── crawl.log.1-3          # Rotated backups
crawler_mcp/crawlers/optimized/README.md:156:- **Automatic Cleanup**: Removes old crawls when limits exceeded
crawler_mcp/crawlers/optimized/README.md:158:- **Domain Organization**: Separate folders per crawled domain
crawler_mcp/crawlers/optimized/README.md:159:- **Backup System**: Previous crawl preserved for comparison
crawler_mcp/crawlers/optimized/README.md:167:--max-crawl-depth 3              # Maximum crawl depth
crawler_mcp/crawlers/optimized/README.md:176:--max-concurrent 28              # Maximum concurrent crawls
crawler_mcp/crawlers/optimized/README.md:221:--verify-qdrant                  # Verify uploads after crawl
crawler_mcp/crawlers/optimized/README.md:230:--enable-cache                   # Enable crawl cache
crawler_mcp/crawlers/optimized/README.md:237:- Coordinates all crawler components
crawler_mcp/crawlers/optimized/README.md:238:- Manages the complete crawl pipeline
crawler_mcp/crawlers/optimized/README.md:242:### Parallel Execution Engine (core/parallel_engine.py)
crawler_mcp/crawlers/optimized/README.md:243:High-performance crawling engine featuring:
crawler_mcp/crawlers/optimized/README.md:332:- **Cache Utilization**: Leverages Crawl4AI caching for repeat crawls
crawler_mcp/crawlers/optimized/README.md:367:The crawler integrates seamlessly with existing environments through:
crawler_mcp/crawlers/optimized/README.md:377:uv run python -m crawler_mcp.crawlers.optimized.run [OPTIONS]
crawler_mcp/crawlers/optimized/README.md:381:- `--url URL`: Target URL to crawl (required unless using search)
crawler_mcp/crawlers/optimized/README.md:382:- `--qdrant-search QUERY`: Semantic search mode (no crawling)
crawler_mcp/crawlers/optimized/README.md:383:- `--verify-qdrant`: Verify Qdrant uploads after crawling
crawler_mcp/crawlers/optimized/README.md:389:- `--max-concurrent N`: Override concurrent crawl limit
crawler_mcp/crawlers/optimized/README.md:397:- `--clean-outputs`: Clean all outputs before starting crawl
crawler_mcp/crawlers/optimized/README.md:408:3. **Custom Strategies**: Create new crawler strategies by inheriting from the base strategy
crawler_mcp/crawlers/optimized/README.md:415:- **Type Checking**: `uv run mypy crawler_mcp`
crawler_mcp/crawlers/optimized/README.md:437:- Check browser factory configuration for proper setup
crawler_mcp/crawlers/optimized/README.md:458:- `--progress`: Monitor real-time crawling progress
crawler_mcp/crawlers/optimized/README.md:465:- `CrawlStats`: Comprehensive crawling statistics
crawler_mcp/crawlers/optimized/README.md:469:- `AsyncCrawlerStrategy`: Main crawler strategy interface
crawler_mcp/crawlers/optimized/README.md:476:- `run_crawl()`: Execute complete crawl pipeline
crawler_mcp/crawlers/optimized/README.md:484:This crawler represents a production-ready solution for large-scale web crawling with advanced features for modern AI and search applications. The modular architecture ensures maintainability while the comprehensive configuration system provides flexibility for diverse use cases.
crawler_mcp/crawlers/optimized/core/__init__.py:2:from .parallel_engine import CrawlStats, ParallelEngine
crawler_mcp/crawlers/optimized/core/batch_utils.py:4:This module provides reusable text batching algorithms extracted from the optimized crawler's
crawler_mcp/crawlers/optimized/core/batch_utils.py:5:strategy.py to avoid code duplication between the crawler and webhook server.
crawler_mcp/crawlers/optimized/core/batch_utils.py:30:    to enable code reuse between the crawler and webhook server.
crawler_mcp/crawlers/optimized/core/adaptive_dispatcher.py:13:from crawl4ai import MemoryAdaptiveDispatcher
crawler_mcp/crawlers/optimized/processing/url_discovery.py:2:URL discovery system for optimized high-performance web crawler.
crawler_mcp/crawlers/optimized/processing/url_discovery.py:4:This module discovers URLs from sitemaps and robots.txt without initial crawling,
crawler_mcp/crawlers/optimized/processing/url_discovery.py:5:enabling intelligent URL prioritization and efficient crawl planning.
crawler_mcp/crawlers/optimized/processing/url_discovery.py:22:    """Discovers URLs from sitemaps and robots.txt without crawling"""
crawler_mcp/crawlers/optimized/processing/url_discovery.py:29:            config: Optional optimized crawler configuration
crawler_mcp/crawlers/optimized/processing/url_discovery.py:84:        Main discovery method that finds URLs without crawling.
crawler_mcp/crawlers/optimized/processing/url_discovery.py:87:        to return a prioritized list of URLs for crawling.
crawler_mcp/crawlers/optimized/processing/url_discovery.py:813:        Check if URL is valid for crawling.
crawler_mcp/crawlers/optimized/processing/url_discovery.py:820:            True if URL is valid for crawling
crawler_mcp/crawlers/optimized/shared/__init__.py:1:"""Shared utilities for optimized crawler CLI (reporting, etc.)."""
crawler_mcp/crawlers/optimized/server.py:5:re-homes middleware and RAG tools under `crawler_mcp.crawlers.optimized`.
crawler_mcp/crawlers/optimized/server.py:34:    from crawler_mcp.config import settings  # type: ignore
crawler_mcp/crawlers/optimized/server.py:35:    from crawler_mcp.core import (  # type: ignore
crawler_mcp/crawlers/optimized/server.py:40:    from crawler_mcp.core.logging import get_logger  # type: ignore
crawler_mcp/crawlers/optimized/server.py:48:    from crawler_mcp.crawlers.optimized.tools.github_pr_tools import register_github_pr_tools  # type: ignore
crawler_mcp/crawlers/optimized/server.py:49:    from crawler_mcp.crawlers.optimized.tools.rag import register_rag_tools  # type: ignore
crawler_mcp/crawlers/optimized/server.py:97:mcp: FastMCP = FastMCP("crawler-mcp")
crawler_mcp/crawlers/optimized/server.py:124:                "max_concurrent_crawls": settings.max_concurrent_crawls,
crawler_mcp/crawlers/optimized/server.py:221:            [sys.executable, "-m", "crawler_mcp.webhook.server"],
crawler_mcp/crawlers/optimized/factories/__init__.py:1:from .browser_factory import BrowserFactory
crawler_mcp/crawlers/optimized/utils/monitoring.py:2:Performance monitoring and hooks for optimized high-performance web crawler.
crawler_mcp/crawlers/optimized/utils/monitoring.py:5:performance tracking, and customizable hooks for the crawling process.
crawler_mcp/crawlers/optimized/utils/monitoring.py:23:    """Container for crawl performance metrics"""
crawler_mcp/crawlers/optimized/utils/monitoring.py:26:    pages_crawled: int = 0
crawler_mcp/crawlers/optimized/utils/monitoring.py:65:    """Monitor crawler performance and provide customizable hooks"""
crawler_mcp/crawlers/optimized/utils/monitoring.py:72:            config: Optional optimized crawler configuration
crawler_mcp/crawlers/optimized/utils/monitoring.py:87:        self._crawl_active = False
crawler_mcp/crawlers/optimized/utils/monitoring.py:103:            "crawl_started",
crawler_mcp/crawlers/optimized/utils/monitoring.py:104:            "crawl_completed",
crawler_mcp/crawlers/optimized/utils/monitoring.py:106:            "page_crawled",
crawler_mcp/crawlers/optimized/utils/monitoring.py:112:            "crawl_paused",
crawler_mcp/crawlers/optimized/utils/monitoring.py:113:            "crawl_resumed",
crawler_mcp/crawlers/optimized/utils/monitoring.py:195:    def start_crawl_monitoring(self, urls: list[str]) -> None:
crawler_mcp/crawlers/optimized/utils/monitoring.py:197:        Start monitoring a crawl session.
crawler_mcp/crawlers/optimized/utils/monitoring.py:200:            urls: List of URLs being crawled
crawler_mcp/crawlers/optimized/utils/monitoring.py:205:        self._crawl_active = True
crawler_mcp/crawlers/optimized/utils/monitoring.py:224:        self.logger.info(f"Started crawl monitoring for {len(urls)} URLs")
crawler_mcp/crawlers/optimized/utils/monitoring.py:225:        self._enqueue_hook("crawl_started", urls=urls, metrics=self.metrics)
crawler_mcp/crawlers/optimized/utils/monitoring.py:228:        self, url: str, content_length: int, crawl_time: float
crawler_mcp/crawlers/optimized/utils/monitoring.py:231:        Record a successful page crawl.
crawler_mcp/crawlers/optimized/utils/monitoring.py:234:            url: URL that was crawled
crawler_mcp/crawlers/optimized/utils/monitoring.py:236:            crawl_time: Time taken to crawl the page
crawler_mcp/crawlers/optimized/utils/monitoring.py:238:        self.metrics.pages_crawled += 1
crawler_mcp/crawlers/optimized/utils/monitoring.py:242:        if self.metrics.pages_crawled == 1:
crawler_mcp/crawlers/optimized/utils/monitoring.py:255:            self.metrics.total_content_bytes / self.metrics.pages_crawled
crawler_mcp/crawlers/optimized/utils/monitoring.py:264:                "crawl_time": crawl_time,
crawler_mcp/crawlers/optimized/utils/monitoring.py:271:            self.metrics.pages_per_second = self.metrics.pages_crawled / elapsed
crawler_mcp/crawlers/optimized/utils/monitoring.py:291:            "page_crawled",
crawler_mcp/crawlers/optimized/utils/monitoring.py:294:            crawl_time=crawl_time,
crawler_mcp/crawlers/optimized/utils/monitoring.py:300:        Record a failed page crawl.
crawler_mcp/crawlers/optimized/utils/monitoring.py:315:        total_attempts = self.metrics.pages_crawled + self.metrics.pages_failed
crawler_mcp/crawlers/optimized/utils/monitoring.py:359:        total_validated = self.metrics.pages_crawled
crawler_mcp/crawlers/optimized/utils/monitoring.py:405:    def finish_crawl_monitoring(self) -> CrawlMetrics:
crawler_mcp/crawlers/optimized/utils/monitoring.py:410:            Final crawl metrics
crawler_mcp/crawlers/optimized/utils/monitoring.py:421:                    self.metrics.pages_crawled / self.metrics.total_duration
crawler_mcp/crawlers/optimized/utils/monitoring.py:424:        self._crawl_active = False
crawler_mcp/crawlers/optimized/utils/monitoring.py:432:            f"Crawl monitoring completed: {self.metrics.pages_crawled} pages, "
crawler_mcp/crawlers/optimized/utils/monitoring.py:437:        self._enqueue_hook("crawl_completed", metrics=self.metrics)
crawler_mcp/crawlers/optimized/utils/monitoring.py:447:            while self._crawl_active:
crawler_mcp/crawlers/optimized/utils/monitoring.py:564:            self.metrics.total_content_bytes / self.metrics.pages_crawled
crawler_mcp/crawlers/optimized/utils/monitoring.py:565:            if self.metrics.pages_crawled > 0
crawler_mcp/crawlers/optimized/utils/monitoring.py:571:                "pages_crawled": self.metrics.pages_crawled,
crawler_mcp/crawlers/optimized/utils/monitoring.py:574:                    self.metrics.pages_crawled
crawler_mcp/crawlers/optimized/utils/monitoring.py:575:                    / (self.metrics.pages_crawled + self.metrics.pages_failed)
crawler_mcp/crawlers/optimized/utils/monitoring.py:576:                    if (self.metrics.pages_crawled + self.metrics.pages_failed) > 0
crawler_mcp/crawlers/optimized/utils/monitoring.py:718:                "Low pages per second. Consider increasing concurrency or optimizing browser config."
crawler_mcp/crawlers/optimized/core/strategy.py:2:Main orchestrator strategy for optimized high-performance web crawler.
crawler_mcp/crawlers/optimized/core/strategy.py:4:This module implements the main crawler strategy that inherits from Crawl4AI's
crawler_mcp/crawlers/optimized/core/strategy.py:20:from crawl4ai.async_crawler_strategy import AsyncCrawlerStrategy
crawler_mcp/crawlers/optimized/core/strategy.py:21:from crawl4ai.models import AsyncCrawlResponse
crawler_mcp/crawlers/optimized/core/strategy.py:23:from ....models.crawl import PageContent
crawler_mcp/crawlers/optimized/core/strategy.py:28:from ..factories.browser_factory import BrowserFactory
crawler_mcp/crawlers/optimized/core/strategy.py:35:from .parallel_engine import ParallelEngine
crawler_mcp/crawlers/optimized/core/strategy.py:39:class OptimizedCrawlerStrategy(AsyncCrawlerStrategy):
crawler_mcp/crawlers/optimized/core/strategy.py:41:    High-performance web crawler strategy using Crawl4AI patterns.
crawler_mcp/crawlers/optimized/core/strategy.py:49:        Initialize optimized crawler strategy.
crawler_mcp/crawlers/optimized/core/strategy.py:61:        self.browser_factory = BrowserFactory(self.config)
crawler_mcp/crawlers/optimized/core/strategy.py:64:        self.parallel_engine = ParallelEngine(self.config)
crawler_mcp/crawlers/optimized/core/strategy.py:101:    async def crawl(self, url: str, **kwargs) -> OptimizedCrawlResponse:
crawler_mcp/crawlers/optimized/core/strategy.py:103:        Main crawl method required by AsyncCrawlerStrategy.
crawler_mcp/crawlers/optimized/core/strategy.py:105:        This method implements the complete optimized crawling pipeline:
crawler_mcp/crawlers/optimized/core/strategy.py:107:        2. Parallel crawling with arun_many()
crawler_mcp/crawlers/optimized/core/strategy.py:112:            url: Starting URL to crawl from
crawler_mcp/crawlers/optimized/core/strategy.py:113:            **kwargs: Additional crawling parameters
crawler_mcp/crawlers/optimized/core/strategy.py:121:            self.logger.info(f"Starting optimized crawl from: {url}")
crawler_mcp/crawlers/optimized/core/strategy.py:123:            # Parse crawling parameters
crawler_mcp/crawlers/optimized/core/strategy.py:126:                "max_concurrent", self.config.max_concurrent_crawls
crawler_mcp/crawlers/optimized/core/strategy.py:132:                    return await self._crawl_github_pr(url, start_time)
crawler_mcp/crawlers/optimized/core/strategy.py:135:                    f"GitHub PR fast-path failed, falling back to normal crawl: {e}"
crawler_mcp/crawlers/optimized/core/strategy.py:156:            # Phase 2: Setup crawling infrastructure
crawler_mcp/crawlers/optimized/core/strategy.py:157:            self.logger.info("Phase 2: Setting up crawling infrastructure")
crawler_mcp/crawlers/optimized/core/strategy.py:159:                browser_config,
crawler_mcp/crawlers/optimized/core/strategy.py:160:                crawler_config,
crawler_mcp/crawlers/optimized/core/strategy.py:165:            self.monitor.start_crawl_monitoring(discovered_urls)
crawler_mcp/crawlers/optimized/core/strategy.py:167:            # Phase 4: Parallel crawling
crawler_mcp/crawlers/optimized/core/strategy.py:169:                f"Phase 4: Starting parallel crawl of {len(discovered_urls)} URLs"
crawler_mcp/crawlers/optimized/core/strategy.py:171:            crawl_results = await self._execute_parallel_crawl(
crawler_mcp/crawlers/optimized/core/strategy.py:173:                browser_config,
crawler_mcp/crawlers/optimized/core/strategy.py:174:                crawler_config,
crawler_mcp/crawlers/optimized/core/strategy.py:182:                crawl_results, discovered_urls, start_time, url
crawler_mcp/crawlers/optimized/core/strategy.py:186:            final_metrics = self.monitor.finish_crawl_monitoring()
crawler_mcp/crawlers/optimized/core/strategy.py:191:                f"Optimized crawl completed: {len(crawl_results)} pages in {duration:.1f}s "
crawler_mcp/crawlers/optimized/core/strategy.py:198:            self.logger.error(f"Optimized crawl failed for {url}: {e}", exc_info=True)
crawler_mcp/crawlers/optimized/core/strategy.py:204:        Initialize crawler resources.
crawler_mcp/crawlers/optimized/core/strategy.py:211:        self.logger.info("Starting optimized crawler strategy")
crawler_mcp/crawlers/optimized/core/strategy.py:218:            self.logger.info("Optimized crawler strategy started successfully")
crawler_mcp/crawlers/optimized/core/strategy.py:221:            self.logger.error(f"Failed to start crawler strategy: {e}")
crawler_mcp/crawlers/optimized/core/strategy.py:226:        Clean up crawler resources.
crawler_mcp/crawlers/optimized/core/strategy.py:233:        self.logger.info("Closing optimized crawler strategy")
crawler_mcp/crawlers/optimized/core/strategy.py:240:            self.logger.info("Optimized crawler strategy closed successfully")
crawler_mcp/crawlers/optimized/core/strategy.py:243:            self.logger.warning(f"Error during crawler strategy cleanup: {e}")
crawler_mcp/crawlers/optimized/core/strategy.py:304:                        fb_browser = self.browser_factory.create_javascript_config()
crawler_mcp/crawlers/optimized/core/strategy.py:306:                        fb_browser = self.browser_factory.get_recommended_config()
crawler_mcp/crawlers/optimized/core/strategy.py:308:                    fb_run_cfg = self.content_extractor.create_crawler_config()
crawler_mcp/crawlers/optimized/core/strategy.py:319:                    first = await self.parallel_engine.crawl_batch_raw(
crawler_mcp/crawlers/optimized/core/strategy.py:320:                        [start_url], fb_browser, fb_run_cfg, None
crawler_mcp/crawlers/optimized/core/strategy.py:492:        Setup crawling infrastructure (browser, config, dispatcher).
crawler_mcp/crawlers/optimized/core/strategy.py:498:            Tuple of (browser_config, crawler_config, dispatcher)
crawler_mcp/crawlers/optimized/core/strategy.py:500:        # Create browser configuration
crawler_mcp/crawlers/optimized/core/strategy.py:501:        browser_config = self.browser_factory.get_recommended_config()
crawler_mcp/crawlers/optimized/core/strategy.py:505:        crawler_config = self.content_extractor.create_crawler_config(markdown_gen)
crawler_mcp/crawlers/optimized/core/strategy.py:512:            f"robots_txt={crawler_config.check_robots_txt}"
crawler_mcp/crawlers/optimized/core/strategy.py:515:        return browser_config, crawler_config, dispatcher
crawler_mcp/crawlers/optimized/core/strategy.py:517:    async def _execute_parallel_crawl(
crawler_mcp/crawlers/optimized/core/strategy.py:520:        browser_config,
crawler_mcp/crawlers/optimized/core/strategy.py:521:        crawler_config,
crawler_mcp/crawlers/optimized/core/strategy.py:527:        Execute parallel crawling of URLs.
crawler_mcp/crawlers/optimized/core/strategy.py:530:            urls: URLs to crawl
crawler_mcp/crawlers/optimized/core/strategy.py:531:            browser_config: Browser configuration
crawler_mcp/crawlers/optimized/core/strategy.py:532:            crawler_config: Crawler configuration
crawler_mcp/crawlers/optimized/core/strategy.py:536:            List of successful crawl results
crawler_mcp/crawlers/optimized/core/strategy.py:544:        # Execute parallel crawl with memory pressure handling
crawler_mcp/crawlers/optimized/core/strategy.py:549:                async for r in self.parallel_engine.crawl_streaming(
crawler_mcp/crawlers/optimized/core/strategy.py:551:                    browser_config,
crawler_mcp/crawlers/optimized/core/strategy.py:552:                    crawler_config,
crawler_mcp/crawlers/optimized/core/strategy.py:560:                results = await self.parallel_engine.crawl_with_retry(
crawler_mcp/crawlers/optimized/core/strategy.py:562:                    browser_config,
crawler_mcp/crawlers/optimized/core/strategy.py:563:                    crawler_config,
crawler_mcp/crawlers/optimized/core/strategy.py:569:                # Standard parallel crawl
crawler_mcp/crawlers/optimized/core/strategy.py:570:                results = await self.parallel_engine.crawl_batch(
crawler_mcp/crawlers/optimized/core/strategy.py:572:                    browser_config,
crawler_mcp/crawlers/optimized/core/strategy.py:573:                    crawler_config,
crawler_mcp/crawlers/optimized/core/strategy.py:580:            self.logger.warning(f"Memory pressure handling kicked in during crawl: {e}")
crawler_mcp/crawlers/optimized/core/strategy.py:581:            # Fall back to reduced concurrency crawling
crawler_mcp/crawlers/optimized/core/strategy.py:590:                    "Falling back to conservative crawling due to memory pressure"
crawler_mcp/crawlers/optimized/core/strategy.py:593:                results = await self.parallel_engine.crawl_batch(
crawler_mcp/crawlers/optimized/core/strategy.py:595:                    browser_config,
crawler_mcp/crawlers/optimized/core/strategy.py:596:                    crawler_config,
crawler_mcp/crawlers/optimized/core/strategy.py:602:                self.logger.error(f"Fallback crawling also failed: {fallback_e}")
crawler_mcp/crawlers/optimized/core/strategy.py:619:                js_browser_config = self.browser_factory.create_javascript_config()
crawler_mcp/crawlers/optimized/core/strategy.py:620:                js_results = await self.parallel_engine.crawl_batch(
crawler_mcp/crawlers/optimized/core/strategy.py:622:                    js_browser_config,
crawler_mcp/crawlers/optimized/core/strategy.py:623:                    crawler_config,  # keep run settings minimal; engine uses None internally
crawler_mcp/crawlers/optimized/core/strategy.py:637:                crawl_time = getattr(result, "crawl_time", 0.0)
crawler_mcp/crawlers/optimized/core/strategy.py:638:                self.monitor.record_page_success(result.url, content_length, crawl_time)
crawler_mcp/crawlers/optimized/core/strategy.py:714:                    to_crawl = link_pool[:budget]
crawler_mcp/crawlers/optimized/core/strategy.py:716:                        f"Fallback link discovery: scheduling {len(to_crawl)} internal links"
crawler_mcp/crawlers/optimized/core/strategy.py:718:                    # Choose JS-enabled browser if requested, as docs/nav often require it
crawler_mcp/crawlers/optimized/core/strategy.py:722:                        fb_browser = self.browser_factory.create_javascript_config()
crawler_mcp/crawlers/optimized/core/strategy.py:724:                        fb_browser = browser_config
crawler_mcp/crawlers/optimized/core/strategy.py:727:                        fb_run_cfg = self.content_extractor.create_crawler_config()
crawler_mcp/crawlers/optimized/core/strategy.py:742:                        fb_run_cfg = crawler_config
crawler_mcp/crawlers/optimized/core/strategy.py:744:                    fb_results = await self.parallel_engine.crawl_batch(
crawler_mcp/crawlers/optimized/core/strategy.py:745:                        to_crawl,
crawler_mcp/crawlers/optimized/core/strategy.py:746:                        fb_browser,
crawler_mcp/crawlers/optimized/core/strategy.py:761:                                crawl_time = getattr(fr, "crawl_time", 0.0)
crawler_mcp/crawlers/optimized/core/strategy.py:763:                                    fr.url, content_length, crawl_time
crawler_mcp/crawlers/optimized/core/strategy.py:865:                    fb_browser = browser_config
crawler_mcp/crawlers/optimized/core/strategy.py:870:                            fb_browser = self.browser_factory.create_javascript_config()
crawler_mcp/crawlers/optimized/core/strategy.py:871:                    fb_results = await self.parallel_engine.crawl_batch(
crawler_mcp/crawlers/optimized/core/strategy.py:873:                        fb_browser,
crawler_mcp/crawlers/optimized/core/strategy.py:874:                        crawler_config,
crawler_mcp/crawlers/optimized/core/strategy.py:900:    async def _crawl_github_pr(self, url: str, start_time: float) -> AsyncCrawlResponse:
crawler_mcp/crawlers/optimized/core/strategy.py:1208:            "pages_crawled": 1,
crawler_mcp/crawlers/optimized/core/strategy.py:1210:            "crawl_type": "github_pr",
crawler_mcp/crawlers/optimized/core/strategy.py:1254:        Process crawl results and create response.
crawler_mcp/crawlers/optimized/core/strategy.py:1259:            start_time: Start time of crawl
crawler_mcp/crawlers/optimized/core/strategy.py:1267:            crawl_result = self.result_converter.batch_to_crawl_result(
crawler_mcp/crawlers/optimized/core/strategy.py:1271:                self._last_pages = list(crawl_result.pages)
crawler_mcp/crawlers/optimized/core/strategy.py:1280:                    and crawl_result.pages
crawler_mcp/crawlers/optimized/core/strategy.py:1283:                        f"Generating embeddings for {len(crawl_result.pages)} pages "
crawler_mcp/crawlers/optimized/core/strategy.py:1286:                    await self._attach_embeddings(crawl_result)
crawler_mcp/crawlers/optimized/core/strategy.py:1310:            for page in crawl_result.pages:
crawler_mcp/crawlers/optimized/core/strategy.py:1331:                "pages_crawled": len(results),
crawler_mcp/crawlers/optimized/core/strategy.py:1333:                "pages_per_second": crawl_result.statistics.pages_per_second,
crawler_mcp/crawlers/optimized/core/strategy.py:1335:                "crawl_type": "optimized_parallel",
crawler_mcp/crawlers/optimized/core/strategy.py:1336:                "duration_seconds": crawl_result.statistics.crawl_duration_seconds,
crawler_mcp/crawlers/optimized/core/strategy.py:1354:                crawl_result.statistics.pages_per_second
crawler_mcp/crawlers/optimized/core/strategy.py:1374:                for page in crawl_result.pages:
crawler_mcp/crawlers/optimized/core/strategy.py:1423:    async def _attach_embeddings(self, crawl_result) -> None:
crawler_mcp/crawlers/optimized/core/strategy.py:1425:        pages = getattr(crawl_result, "pages", []) or []
crawler_mcp/crawlers/optimized/core/strategy.py:1606:                        f"Upserting {len(crawl_result.pages)} pages to Qdrant "
crawler_mcp/crawlers/optimized/core/strategy.py:1609:                    await self._upsert_qdrant(crawl_result, vector_dim=dim)
crawler_mcp/crawlers/optimized/core/strategy.py:1615:    async def _upsert_qdrant(self, crawl_result, *, vector_dim: int) -> None:
crawler_mcp/crawlers/optimized/core/strategy.py:1616:        pages = getattr(crawl_result, "pages", []) or []
crawler_mcp/crawlers/optimized/core/strategy.py:1620:        collection = getattr(self.config, "qdrant_collection", "crawler_pages")
crawler_mcp/crawlers/optimized/core/strategy.py:1727:        Create error response for failed crawls.
crawler_mcp/crawlers/optimized/core/strategy.py:1740:            "pages_crawled": 0,
crawler_mcp/crawlers/optimized/core/strategy.py:1742:            "crawl_type": "failed",
crawler_mcp/crawlers/optimized/core/strategy.py:1768:            url: The URL being crawled
crawler_mcp/crawlers/optimized/core/strategy.py:1830:        """Return the PageContent list from the last crawl, if available."""
crawler_mcp/crawlers/optimized/core/strategy.py:1844:        self.browser_factory = BrowserFactory(self.config)
crawler_mcp/crawlers/optimized/core/strategy.py:1847:        self.parallel_engine = ParallelEngine(self.config)
crawler_mcp/crawlers/optimized/core/strategy.py:1853:    async def crawl_single_url(self, url: str, **kwargs) -> dict[str, Any]:
crawler_mcp/crawlers/optimized/core/strategy.py:1857:        This is a convenience method for single URL crawling that returns
crawler_mcp/crawlers/optimized/core/strategy.py:1861:            url: URL to crawl
crawler_mcp/crawlers/optimized/core/strategy.py:1865:            Dictionary with crawl results in our format
crawler_mcp/crawlers/optimized/core/strategy.py:1873:            # Use the main crawl method
crawler_mcp/crawlers/optimized/core/strategy.py:1874:            response = await self.crawl(url, **kwargs)
crawler_mcp/crawlers/optimized/core/strategy.py:1896:                    "pages_crawled": int(headers.get("X-Pages-Crawled", "0")),
crawler_mcp/crawlers/optimized/core/strategy.py:1922:            f"concurrent={self.config.max_concurrent_crawls}, "
crawler_mcp/crawlers/optimized/models/__init__.py:2:Models for optimized crawler responses and data structures.
crawler_mcp/crawlers/optimized/factories/dispatcher_factory.py:2:Memory-adaptive dispatcher factory for optimized high-performance web crawler.
crawler_mcp/crawlers/optimized/factories/dispatcher_factory.py:5:optimized for different crawling scenarios and hardware configurations.
crawler_mcp/crawlers/optimized/factories/dispatcher_factory.py:10:from crawl4ai import MemoryAdaptiveDispatcher
crawler_mcp/crawlers/optimized/factories/dispatcher_factory.py:23:            config: Optional optimized crawler configuration
crawler_mcp/crawlers/optimized/factories/dispatcher_factory.py:37:            max_concurrent: Maximum concurrent crawls (defaults to config)
crawler_mcp/crawlers/optimized/factories/dispatcher_factory.py:44:        concurrent = max_concurrent or self.config.max_concurrent_crawls
crawler_mcp/crawlers/optimized/factories/dispatcher_factory.py:60:            max_concurrent: Maximum concurrent crawls (defaults to config * 1.5)
crawler_mcp/crawlers/optimized/factories/dispatcher_factory.py:66:        concurrent = max_concurrent or (self.config.max_concurrent_crawls * 1.5)
crawler_mcp/crawlers/optimized/factories/dispatcher_factory.py:82:            max_concurrent: Maximum concurrent crawls (defaults to config * 0.75)
crawler_mcp/crawlers/optimized/factories/dispatcher_factory.py:88:            1, int(self.config.max_concurrent_crawls * 0.75)
crawler_mcp/crawlers/optimized/factories/dispatcher_factory.py:104:            max_concurrent: Maximum concurrent crawls (defaults to config // 2)
crawler_mcp/crawlers/optimized/factories/dispatcher_factory.py:109:        concurrent = max_concurrent or max(1, self.config.max_concurrent_crawls // 2)
crawler_mcp/crawlers/optimized/factories/dispatcher_factory.py:121:        Create dispatcher for large-scale crawling operations.
crawler_mcp/crawlers/optimized/factories/dispatcher_factory.py:124:            max_concurrent: Maximum concurrent crawls (defaults to config * 2)
crawler_mcp/crawlers/optimized/factories/dispatcher_factory.py:129:        concurrent = max_concurrent or (self.config.max_concurrent_crawls * 2)
crawler_mcp/crawlers/optimized/factories/dispatcher_factory.py:166:            max_concurrent: Maximum concurrent crawls
crawler_mcp/crawlers/optimized/cli/__init__.py:1:"""CLI modules for the optimized crawler."""
crawler_mcp/crawlers/optimized/processing/result_converter.py:2:Result conversion utilities for optimized high-performance web crawler.
crawler_mcp/crawlers/optimized/processing/result_converter.py:14:from crawl4ai.models import CrawlResult as Crawl4AIResult
crawler_mcp/crawlers/optimized/processing/result_converter.py:17:from ....models.crawl import (
crawler_mcp/crawlers/optimized/processing/result_converter.py:24:from ..core.parallel_engine import CrawlStats
crawler_mcp/crawlers/optimized/processing/result_converter.py:35:            config: Optional optimized crawler configuration
crawler_mcp/crawlers/optimized/processing/result_converter.py:40:    def crawl4ai_to_page_content(
crawler_mcp/crawlers/optimized/processing/result_converter.py:47:            result: Crawl4AI crawl result
crawler_mcp/crawlers/optimized/processing/result_converter.py:236:            # Add basic crawl information
crawler_mcp/crawlers/optimized/processing/result_converter.py:241:                    "extraction_method": "crawl4ai_optimized",
crawler_mcp/crawlers/optimized/processing/result_converter.py:242:                    "crawl_timestamp": datetime.now().isoformat(),
crawler_mcp/crawlers/optimized/processing/result_converter.py:247:            if hasattr(result, "crawl_time"):
crawler_mcp/crawlers/optimized/processing/result_converter.py:248:                metadata["crawl_time"] = result.crawl_time
crawler_mcp/crawlers/optimized/processing/result_converter.py:316:    def batch_to_crawl_result(
crawler_mcp/crawlers/optimized/processing/result_converter.py:318:        crawl4ai_results: list[Crawl4AIResult],
crawler_mcp/crawlers/optimized/processing/result_converter.py:328:            crawl4ai_results: List of Crawl4AI results
crawler_mcp/crawlers/optimized/processing/result_converter.py:330:            start_time: Start time of the crawl session
crawler_mcp/crawlers/optimized/processing/result_converter.py:340:            for result in crawl4ai_results:
crawler_mcp/crawlers/optimized/processing/result_converter.py:341:                page_content = self.crawl4ai_to_page_content(result)
crawler_mcp/crawlers/optimized/processing/result_converter.py:384:        """Calculate crawl statistics"""
crawler_mcp/crawlers/optimized/processing/result_converter.py:388:            total_crawled = len(pages)
crawler_mcp/crawlers/optimized/processing/result_converter.py:413:            pages_per_second = total_crawled / duration if duration > 0 else 0
crawler_mcp/crawlers/optimized/processing/result_converter.py:414:            average_page_size = total_bytes / total_crawled if total_crawled > 0 else 0
crawler_mcp/crawlers/optimized/processing/result_converter.py:418:                total_pages_crawled=total_crawled,
crawler_mcp/crawlers/optimized/processing/result_converter.py:423:                crawl_duration_seconds=duration,
crawler_mcp/crawlers/optimized/processing/result_converter.py:432:    def crawl_stats_to_statistics(self, stats: CrawlStats) -> CrawlStatistics:
crawler_mcp/crawlers/optimized/processing/result_converter.py:444:            total_pages_crawled=stats.urls_successful,
crawler_mcp/crawlers/optimized/processing/result_converter.py:449:            crawl_duration_seconds=stats.total_duration,
crawler_mcp/crawlers/optimized/processing/result_converter.py:467:        metadata = {"crawl_failed": True}
crawler_mcp/crawlers/optimized/processing/result_converter.py:487:        self, crawl4ai_result: Crawl4AIResult, page_content: PageContent
crawler_mcp/crawlers/optimized/processing/result_converter.py:493:            crawl4ai_result: Original Crawl4AI result
crawler_mcp/crawlers/optimized/processing/result_converter.py:501:            if page_content.url != crawl4ai_result.url:
crawler_mcp/crawlers/optimized/processing/result_converter.py:505:            if crawl4ai_result.success and not page_content.content:
crawler_mcp/crawlers/optimized/factories/browser_factory.py:2:Browser configuration factory for optimized high-performance web crawler.
crawler_mcp/crawlers/optimized/factories/browser_factory.py:4:This module provides factory methods for creating optimized browser configurations
crawler_mcp/crawlers/optimized/factories/browser_factory.py:5:for different crawling scenarios, focusing on performance and resource efficiency.
crawler_mcp/crawlers/optimized/factories/browser_factory.py:8:from crawl4ai import BrowserConfig
crawler_mcp/crawlers/optimized/factories/browser_factory.py:14:    """Factory for creating optimized browser configurations"""
crawler_mcp/crawlers/optimized/factories/browser_factory.py:18:        Initialize browser factory.
crawler_mcp/crawlers/optimized/factories/browser_factory.py:21:            config: Optional optimized crawler configuration
crawler_mcp/crawlers/optimized/factories/browser_factory.py:27:        Create high-performance browser configuration optimized for speed.
crawler_mcp/crawlers/optimized/factories/browser_factory.py:29:        This configuration is optimized for maximum crawling throughput with:
crawler_mcp/crawlers/optimized/factories/browser_factory.py:32:        - Minimal browser features enabled
crawler_mcp/crawlers/optimized/factories/browser_factory.py:61:            # Prefer blocking via crawler config; use blink setting as a hint only
crawler_mcp/crawlers/optimized/factories/browser_factory.py:65:            headless=self.config.browser_headless,
crawler_mcp/crawlers/optimized/factories/browser_factory.py:66:            browser_type="chromium",  # Use Chromium for best performance
crawler_mcp/crawlers/optimized/factories/browser_factory.py:71:            light_mode=True,  # Reduce browser features
crawler_mcp/crawlers/optimized/factories/browser_factory.py:80:        Create browser configuration with JavaScript enabled for dynamic content.
crawler_mcp/crawlers/optimized/factories/browser_factory.py:98:        Create browser configuration with anti-detection features.
crawler_mcp/crawlers/optimized/factories/browser_factory.py:100:        This configuration attempts to avoid detection as an automated browser
crawler_mcp/crawlers/optimized/factories/browser_factory.py:112:            # Appear more like a real browser
crawler_mcp/crawlers/optimized/factories/browser_factory.py:119:            browser_type="chromium",
crawler_mcp/crawlers/optimized/factories/browser_factory.py:131:        Create ultra-minimal browser configuration for maximum speed.
crawler_mcp/crawlers/optimized/factories/browser_factory.py:141:            browser_type="chromium",
crawler_mcp/crawlers/optimized/factories/browser_factory.py:162:        multiple browser instances will be running simultaneously.
crawler_mcp/crawlers/optimized/factories/browser_factory.py:183:        Create browser configuration for specific crawling scenarios.
crawler_mcp/crawlers/optimized/factories/browser_factory.py:219:        Customize an existing browser configuration.
crawler_mcp/crawlers/optimized/factories/browser_factory.py:223:            custom_args: Additional browser arguments
crawler_mcp/crawlers/optimized/factories/browser_factory.py:233:            browser_type=base_config.browser_type,
crawler_mcp/crawlers/optimized/models/responses.py:2:Custom response models for optimized crawler.
crawler_mcp/crawlers/optimized/models/responses.py:11:from crawl4ai.models import AsyncCrawlResponse
crawler_mcp/crawlers/optimized/models/responses.py:18:    Extended AsyncCrawlResponse with additional attributes for optimized crawler.
crawler_mcp/crawlers/optimized/models/responses.py:21:    - success: Boolean indicating crawl success
crawler_mcp/crawlers/optimized/models/responses.py:23:    - metadata: Dictionary with crawl statistics and information
crawler_mcp/crawlers/optimized/models/responses.py:47:    def from_async_crawl_response(
crawler_mcp/crawlers/optimized/models/responses.py:59:            success: Whether the crawl was successful
crawler_mcp/crawlers/optimized/models/responses.py:61:            metadata: Additional metadata about the crawl
crawler_mcp/crawlers/optimized/factories/content_extractor.py:2:Content extraction factory for optimized high-performance web crawler.
crawler_mcp/crawlers/optimized/factories/content_extractor.py:8:from crawl4ai import CacheMode, CrawlerRunConfig, DefaultMarkdownGenerator
crawler_mcp/crawlers/optimized/factories/content_extractor.py:9:from crawl4ai.content_filter_strategy import PruningContentFilter
crawler_mcp/crawlers/optimized/factories/content_extractor.py:22:            config: Optional optimized crawler configuration
crawler_mcp/crawlers/optimized/factories/content_extractor.py:53:            "skip_internal_links": False,  # Keep internal links for crawling
crawler_mcp/crawlers/optimized/factories/content_extractor.py:132:    def create_crawler_config(
crawler_mcp/crawlers/optimized/factories/content_extractor.py:139:        Create crawler configuration with content extraction settings.
crawler_mcp/crawlers/optimized/factories/content_extractor.py:240:        Create configuration focused on maximum crawling speed.
crawler_mcp/crawlers/optimized/factories/content_extractor.py:303:            "general": self.create_crawler_config,
crawler_mcp/crawlers/optimized/factories/content_extractor.py:333:        return self.create_crawler_config(
crawler_mcp/crawlers/optimized/factories/content_extractor.py:344:        config = self.create_crawler_config(
crawler_mcp/crawlers/optimized/factories/content_extractor.py:366:        return self.create_crawler_config(
crawler_mcp/crawlers/optimized/factories/content_extractor.py:389:        return self.create_crawler_config(
crawler_mcp/crawlers/optimized/factories/content_extractor.py:411:        return self.create_crawler_config(
crawler_mcp/crawlers/optimized/core/parallel_engine.py:2:Parallel crawling engine for optimized high-performance web crawler.
crawler_mcp/crawlers/optimized/core/parallel_engine.py:4:This module implements high-performance parallel crawling using crawl4ai's arun_many()
crawler_mcp/crawlers/optimized/core/parallel_engine.py:18:from crawl4ai import AsyncWebCrawler, BrowserConfig, CrawlerRunConfig
crawler_mcp/crawlers/optimized/core/parallel_engine.py:19:from crawl4ai.models import CrawlResult
crawler_mcp/crawlers/optimized/core/parallel_engine.py:28:    """Statistics for a crawling session"""
crawler_mcp/crawlers/optimized/core/parallel_engine.py:41:    """High-performance parallel crawling engine using arun_many()"""
crawler_mcp/crawlers/optimized/core/parallel_engine.py:45:        Initialize parallel crawling engine.
crawler_mcp/crawlers/optimized/core/parallel_engine.py:48:            config: Optional optimized crawler configuration
crawler_mcp/crawlers/optimized/core/parallel_engine.py:53:    async def crawl_batch(
crawler_mcp/crawlers/optimized/core/parallel_engine.py:56:        browser_config: BrowserConfig,
crawler_mcp/crawlers/optimized/core/parallel_engine.py:57:        crawler_config: CrawlerRunConfig,
crawler_mcp/crawlers/optimized/core/parallel_engine.py:65:        This is the main method for high-performance parallel crawling that uses
crawler_mcp/crawlers/optimized/core/parallel_engine.py:66:        crawl4ai's arun_many() with proper content validation.
crawler_mcp/crawlers/optimized/core/parallel_engine.py:69:            urls: List of URLs to crawl
crawler_mcp/crawlers/optimized/core/parallel_engine.py:70:            browser_config: Browser configuration
crawler_mcp/crawlers/optimized/core/parallel_engine.py:71:            crawler_config: Crawler run configuration
crawler_mcp/crawlers/optimized/core/parallel_engine.py:86:        self.logger.info(f"Starting parallel crawl of {len(urls)} URLs")
crawler_mcp/crawlers/optimized/core/parallel_engine.py:90:        batch_config = self._prepare_batch_config(crawler_config)
crawler_mcp/crawlers/optimized/core/parallel_engine.py:95:            async with AsyncWebCrawler(config=browser_config) as crawler:
crawler_mcp/crawlers/optimized/core/parallel_engine.py:97:                await self._enable_resource_blocking(crawler)
crawler_mcp/crawlers/optimized/core/parallel_engine.py:106:                                self.config.max_concurrent_crawls,
crawler_mcp/crawlers/optimized/core/parallel_engine.py:110:                        max_conc = self.config.max_concurrent_crawls
crawler_mcp/crawlers/optimized/core/parallel_engine.py:122:                results_generator = await crawler.arun_many(
crawler_mcp/crawlers/optimized/core/parallel_engine.py:208:            self.logger.error(f"Parallel crawl failed: {e}")
crawler_mcp/crawlers/optimized/core/parallel_engine.py:220:            f"Parallel crawl completed: {len(successful_results)} successful, "
crawler_mcp/crawlers/optimized/core/parallel_engine.py:227:    async def crawl_batch_raw(
crawler_mcp/crawlers/optimized/core/parallel_engine.py:230:        browser_config: BrowserConfig,
crawler_mcp/crawlers/optimized/core/parallel_engine.py:231:        crawler_config: CrawlerRunConfig,
crawler_mcp/crawlers/optimized/core/parallel_engine.py:245:        batch_config = self._prepare_batch_config(crawler_config)
crawler_mcp/crawlers/optimized/core/parallel_engine.py:248:            async with AsyncWebCrawler(config=browser_config) as crawler:
crawler_mcp/crawlers/optimized/core/parallel_engine.py:249:                await self._enable_resource_blocking(crawler)
crawler_mcp/crawlers/optimized/core/parallel_engine.py:250:                gen = await crawler.arun_many(
crawler_mcp/crawlers/optimized/core/parallel_engine.py:262:            self.logger.debug(f"crawl_batch_raw failed: {e}")
crawler_mcp/crawlers/optimized/core/parallel_engine.py:265:    async def crawl_streaming(
crawler_mcp/crawlers/optimized/core/parallel_engine.py:268:        browser_config: BrowserConfig,
crawler_mcp/crawlers/optimized/core/parallel_engine.py:269:        crawler_config: CrawlerRunConfig,
crawler_mcp/crawlers/optimized/core/parallel_engine.py:275:        Stream crawl results as they become available.
crawler_mcp/crawlers/optimized/core/parallel_engine.py:278:        real-time processing of crawled content.
crawler_mcp/crawlers/optimized/core/parallel_engine.py:281:            urls: List of URLs to crawl
crawler_mcp/crawlers/optimized/core/parallel_engine.py:282:            browser_config: Browser configuration
crawler_mcp/crawlers/optimized/core/parallel_engine.py:283:            crawler_config: Crawler run configuration
crawler_mcp/crawlers/optimized/core/parallel_engine.py:293:        self.logger.info(f"Starting streaming crawl of {len(urls)} URLs")
crawler_mcp/crawlers/optimized/core/parallel_engine.py:296:        batch_config = self._prepare_batch_config(crawler_config)
crawler_mcp/crawlers/optimized/core/parallel_engine.py:302:            async with AsyncWebCrawler(config=browser_config) as crawler:
crawler_mcp/crawlers/optimized/core/parallel_engine.py:303:                await self._enable_resource_blocking(crawler)
crawler_mcp/crawlers/optimized/core/parallel_engine.py:310:                                self.config.max_concurrent_crawls,
crawler_mcp/crawlers/optimized/core/parallel_engine.py:314:                        max_conc = self.config.max_concurrent_crawls
crawler_mcp/crawlers/optimized/core/parallel_engine.py:323:                results_generator = await crawler.arun_many(
crawler_mcp/crawlers/optimized/core/parallel_engine.py:361:            self.logger.error(f"Streaming crawl failed: {e}")
crawler_mcp/crawlers/optimized/core/parallel_engine.py:366:    async def crawl_with_retry(
crawler_mcp/crawlers/optimized/core/parallel_engine.py:369:        browser_config: BrowserConfig,
crawler_mcp/crawlers/optimized/core/parallel_engine.py:370:        crawler_config: CrawlerRunConfig,
crawler_mcp/crawlers/optimized/core/parallel_engine.py:380:            urls: List of URLs to crawl
crawler_mcp/crawlers/optimized/core/parallel_engine.py:381:            browser_config: Browser configuration
crawler_mcp/crawlers/optimized/core/parallel_engine.py:382:            crawler_config: Crawler run configuration
crawler_mcp/crawlers/optimized/core/parallel_engine.py:397:            attempt_results = await self.crawl_batch(
crawler_mcp/crawlers/optimized/core/parallel_engine.py:399:                browser_config,
crawler_mcp/crawlers/optimized/core/parallel_engine.py:400:                crawler_config,
crawler_mcp/crawlers/optimized/core/parallel_engine.py:420:                f"Failed to crawl {len(urls_to_retry)} URLs after {max_retries + 1} attempts"
crawler_mcp/crawlers/optimized/core/parallel_engine.py:425:    async def _enable_resource_blocking(self, crawler: Any) -> None:
crawler_mcp/crawlers/optimized/core/parallel_engine.py:435:            for attr in ("context", "_context", "browser_context"):
crawler_mcp/crawlers/optimized/core/parallel_engine.py:436:                context = getattr(crawler, attr, None)
crawler_mcp/crawlers/optimized/core/parallel_engine.py:443:            page = getattr(crawler, "page", None) or getattr(crawler, "_page", None)
crawler_mcp/crawlers/optimized/core/parallel_engine.py:475:    async def crawl_batched(
crawler_mcp/crawlers/optimized/core/parallel_engine.py:478:        browser_config: BrowserConfig,
crawler_mcp/crawlers/optimized/core/parallel_engine.py:479:        crawler_config: CrawlerRunConfig,
crawler_mcp/crawlers/optimized/core/parallel_engine.py:487:            urls: List of URLs to crawl
crawler_mcp/crawlers/optimized/core/parallel_engine.py:488:            browser_config: Browser configuration
crawler_mcp/crawlers/optimized/core/parallel_engine.py:489:            crawler_config: Crawler run configuration
crawler_mcp/crawlers/optimized/core/parallel_engine.py:515:                batch_results = await self.crawl_batch(
crawler_mcp/crawlers/optimized/core/parallel_engine.py:516:                    batch_urls, browser_config, crawler_config, dispatcher
crawler_mcp/crawlers/optimized/core/parallel_engine.py:535:        self, crawler_config: CrawlerRunConfig
crawler_mcp/crawlers/optimized/core/parallel_engine.py:538:        Prepare crawler configuration optimized for batch processing.
crawler_mcp/crawlers/optimized/core/parallel_engine.py:541:            crawler_config: Base crawler configuration
crawler_mcp/crawlers/optimized/core/parallel_engine.py:544:            Optimized crawler configuration for batch processing
crawler_mcp/crawlers/optimized/core/parallel_engine.py:547:        batch_config = self._clone_config(crawler_config)
crawler_mcp/crawlers/optimized/core/parallel_engine.py:565:        """Create a cautious copy of crawler configuration.
crawler_mcp/crawlers/optimized/core/parallel_engine.py:618:        Validate crawl result content quality.
crawler_mcp/crawlers/optimized/core/parallel_engine.py:770:        Calculate statistics for a crawling session.
crawler_mcp/crawlers/optimized/core/parallel_engine.py:774:            duration: Total crawling duration in seconds
crawler_mcp/crawlers/optimized/shared/reporting.py:11:    """Pretty-print the enhanced crawl report and return possibly augmented report.
crawler_mcp/crawlers/optimized/shared/reporting.py:64:    pages_crawled = _hdr("X-Pages-Crawled", "0")
crawler_mcp/crawlers/optimized/shared/reporting.py:83:    print(KEY("- 📄 Pages:"), VAL(f"{pages_crawled} / {urls_discovered}"))
crawler_mcp/crawlers/optimized/shared/reporting.py:218:            def _page_crawl_time(p):
crawler_mcp/crawlers/optimized/shared/reporting.py:221:                    return float(md.get("crawl_time", 0.0))
crawler_mcp/crawlers/optimized/shared/reporting.py:225:            slowest = sorted(pages, key=_page_crawl_time, reverse=True)[:5]
crawler_mcp/crawlers/optimized/shared/reporting.py:226:            if slowest and _page_crawl_time(slowest[0]) > 0:
crawler_mcp/crawlers/optimized/shared/reporting.py:229:                    ct = _page_crawl_time(pg)
crawler_mcp/crawlers/optimized/shared/reporting.py:242:        print(KEY("- 📄 Pages Crawled:"), VAL(str(summ.get("pages_crawled", 0))))
crawler_mcp/crawlers/optimized/cli/crawl.py:2:CLI runner for the optimized crawler.
crawler_mcp/crawlers/optimized/cli/crawl.py:5:  uv run python -m crawler_mcp.crawlers.optimized.run \
crawler_mcp/crawlers/optimized/cli/crawl.py:7:      --output-html crawl_all.html \
crawler_mcp/crawlers/optimized/cli/crawl.py:34:    p = argparse.ArgumentParser("optimized-crawler")
crawler_mcp/crawlers/optimized/cli/crawl.py:39:        help="Start URL to crawl (required unless using --qdrant-search)",
crawler_mcp/crawlers/optimized/cli/crawl.py:41:    p.add_argument("--max-urls", type=int, default=1000, help="Max URLs to crawl")
crawler_mcp/crawlers/optimized/cli/crawl.py:42:    p.add_argument("--concurrency", type=int, default=16, help="Max concurrent crawls")
crawler_mcp/crawlers/optimized/cli/crawl.py:68:        help="Clean all outputs before starting crawl",
crawler_mcp/crawlers/optimized/cli/crawl.py:95:        help="Enable JavaScript during crawl (default: use config)",
crawler_mcp/crawlers/optimized/cli/crawl.py:212:        help="After crawl, verify Qdrant collection count and a sample vector",
crawler_mcp/crawlers/optimized/cli/crawl.py:322:        help="Show periodic progress (pps, CPU, memory) while crawling",
crawler_mcp/crawlers/optimized/cli/crawl.py:370:            "optimized-crawler: error: --url is required unless using --qdrant-search",
crawler_mcp/crawlers/optimized/cli/crawl.py:426:    cfg.max_concurrent_crawls = args.concurrency
crawler_mcp/crawlers/optimized/cli/crawl.py:440:        cfg.max_concurrent_crawls = max(cfg.max_concurrent_crawls, 28)
crawler_mcp/crawlers/optimized/cli/crawl.py:800:    crawl_logger = log_mgr.setup_crawl_logger()
crawler_mcp/crawlers/optimized/cli/crawl.py:813:    # Rotate backup before crawl (unless --no-backup)
crawler_mcp/crawlers/optimized/cli/crawl.py:815:        output_mgr.rotate_crawl_backup(domain)
crawler_mcp/crawlers/optimized/cli/crawl.py:817:    crawl_logger.info(f"Starting crawl for {args.url}")
crawler_mcp/crawlers/optimized/cli/crawl.py:818:    crawl_logger.info(f"Output directory: {args.output_dir}")
crawler_mcp/crawlers/optimized/cli/crawl.py:819:    crawl_logger.info(f"Domain: {domain}")
crawler_mcp/crawlers/optimized/cli/crawl.py:832:        def on_crawl_started(urls=None, metrics=None, **_):  # type: ignore[no-redef]
crawler_mcp/crawlers/optimized/cli/crawl.py:838:        def on_page_crawled(url=None, content_length=0, crawl_time=0.0, **_):  # type: ignore[no-redef]
crawler_mcp/crawlers/optimized/cli/crawl.py:839:            ms = int(float(crawl_time) * 1000) if crawl_time else 0
crawler_mcp/crawlers/optimized/cli/crawl.py:849:        strat.set_hook("crawl_started", on_crawl_started)
crawler_mcp/crawlers/optimized/cli/crawl.py:850:        strat.set_hook("page_crawled", on_page_crawled)
crawler_mcp/crawlers/optimized/cli/crawl.py:872:                    pc = int(metrics.pages_crawled) if metrics else 0
crawler_mcp/crawlers/optimized/cli/crawl.py:894:        crawl_logger.info(f"Beginning crawl of {args.url}")
crawler_mcp/crawlers/optimized/cli/crawl.py:895:        resp = await strat.crawl(
crawler_mcp/crawlers/optimized/cli/crawl.py:898:        crawl_logger.info("Crawl completed successfully")
crawler_mcp/crawlers/optimized/cli/crawl.py:923:                output_mgr.save_crawl_outputs(domain, resp.html, page_data, report)
crawler_mcp/crawlers/optimized/cli/crawl.py:930:                        "pages_crawled": len(page_data),
crawler_mcp/crawlers/optimized/cli/crawl.py:937:                crawl_logger.info(
crawler_mcp/crawlers/optimized/cli/crawl.py:938:                    f"Saved outputs to {args.output_dir}/crawls/{domain}/latest/"
crawler_mcp/crawlers/optimized/cli/crawl.py:941:                    f"✅ Outputs saved to: {args.output_dir}/crawls/{domain}/latest/"
crawler_mcp/crawlers/optimized/cli/crawl.py:948:            crawl_logger.info("Skipping output save (--skip-output)")
crawler_mcp/crawlers/optimized/cli/crawl.py:1071:                    output_mgr.save_crawl_outputs(domain, None, pages, report)
crawler_mcp/crawlers/optimized/cli/crawl.py:1076:                            "pages_crawled": len(pages),
crawler_mcp/crawlers/optimized/cli/pr.py:6:  uv run python -m crawler_mcp.crawlers.optimized.cli.pr pr-list \
crawler_mcp/crawlers/optimized/cli/pr.py:11:  uv run python -m crawler_mcp.crawlers.optimized.cli.pr pr-context \
crawler_mcp/crawlers/optimized/cli/pr.py:15:  uv run python -m crawler_mcp.crawlers.optimized.cli.pr pr-apply-suggestions \
crawler_mcp/crawlers/optimized/cli/pr.py:19:  uv run python -m crawler_mcp.crawlers.optimized.cli.pr pr-branch \
crawler_mcp/crawlers/optimized/cli/pr.py:23:  uv run python -m crawler_mcp.crawlers.optimized.cli.pr pr-comment \
crawler_mcp/crawlers/optimized/cli/pr.py:27:  uv run python -m crawler_mcp.crawlers.optimized.cli.pr pr-mark-resolved \

Models mentioning crawl:*
crawler_mcp/config.py:17:class CrawlerMCPSettings(BaseSettings):
crawler_mcp/crawlers/optimized/processing/result_converter.py:14:from crawl4ai.models import CrawlResult as Crawl4AIResult
crawler_mcp/crawlers/optimized/processing/result_converter.py:18:    CrawlResult,
crawler_mcp/crawlers/optimized/processing/result_converter.py:323:    ) -> CrawlResult:
crawler_mcp/crawlers/optimized/processing/result_converter.py:325:        Convert batch of Crawl4AI results to our CrawlResult model.
crawler_mcp/crawlers/optimized/processing/result_converter.py:335:            CrawlResult compatible with our existing system
crawler_mcp/crawlers/optimized/processing/result_converter.py:355:            return CrawlResult(
crawler_mcp/crawlers/optimized/processing/result_converter.py:368:            return CrawlResult(
crawler_mcp/crawlers/optimized/models/responses.py:16:class OptimizedCrawlResponse(AsyncCrawlResponse):
crawler_mcp/crawlers/optimized/utils/monitoring.py:22:class CrawlMetrics:
crawler_mcp/crawlers/optimized/core/parallel_engine.py:19:from crawl4ai.models import CrawlResult
crawler_mcp/crawlers/optimized/core/parallel_engine.py:27:class CrawlStats:
crawler_mcp/crawlers/optimized/core/parallel_engine.py:61:    ) -> list[CrawlResult]:
crawler_mcp/crawlers/optimized/core/parallel_engine.py:76:            List of successful CrawlResult objects
crawler_mcp/crawlers/optimized/core/parallel_engine.py:233:    ) -> list[CrawlResult]:
crawler_mcp/crawlers/optimized/core/parallel_engine.py:235:        Crawl multiple URLs and return raw CrawlResult objects without content validation.
crawler_mcp/crawlers/optimized/core/parallel_engine.py:243:        results: list[CrawlResult] = []
crawler_mcp/crawlers/optimized/core/parallel_engine.py:272:        result_callback: Callable[[CrawlResult], None] | None = None,
crawler_mcp/crawlers/optimized/core/parallel_engine.py:273:    ) -> AsyncGenerator[CrawlResult, None]:
crawler_mcp/crawlers/optimized/core/parallel_engine.py:288:            CrawlResult objects as they become available
crawler_mcp/crawlers/optimized/core/parallel_engine.py:375:    ) -> list[CrawlResult]:
crawler_mcp/crawlers/optimized/core/parallel_engine.py:388:            List of successful CrawlResult objects
crawler_mcp/crawlers/optimized/core/parallel_engine.py:482:    ) -> list[CrawlResult]:
crawler_mcp/crawlers/optimized/core/parallel_engine.py:494:            List of successful CrawlResult objects from all batches
crawler_mcp/crawlers/optimized/core/parallel_engine.py:615:        self, result: CrawlResult, monitor: PerformanceMonitor | None = None
crawler_mcp/crawlers/optimized/core/parallel_engine.py:624:            result: CrawlResult to validate
crawler_mcp/crawlers/optimized/core/parallel_engine.py:684:    def _extract_content_for_validation(self, result: CrawlResult) -> str:
crawler_mcp/crawlers/optimized/core/parallel_engine.py:767:        self, results: list[CrawlResult], duration: float, total_requested: int
crawler_mcp/crawlers/optimized/core/strategy.py:39:class OptimizedCrawlerStrategy(AsyncCrawlerStrategy):
crawler_mcp/crawlers/directory.py:15:    CrawlResult,
crawler_mcp/crawlers/directory.py:41:class DirectoryCrawlStrategy(BaseCrawlStrategy):
crawler_mcp/crawlers/directory.py:67:    ) -> CrawlResult:
crawler_mcp/crawlers/directory.py:89:                return CrawlResult(
crawler_mcp/crawlers/directory.py:158:            # success_rate calculated as property on CrawlResult
crawler_mcp/crawlers/directory.py:172:            result = CrawlResult(
crawler_mcp/crawlers/directory.py:191:            return CrawlResult(
crawler_mcp/crawlers/web.py:19:from crawl4ai import CrawlResult as Crawl4aiResult  # type: ignore
crawler_mcp/crawlers/web.py:32:    CrawlRequest,
crawler_mcp/crawlers/web.py:33:    CrawlResult,
crawler_mcp/crawlers/web.py:45:class WebCrawlStrategy(BaseCrawlStrategy):
crawler_mcp/crawlers/web.py:160:    async def validate_request(self, request: CrawlRequest) -> bool:
crawler_mcp/crawlers/web.py:186:        request: CrawlRequest,
crawler_mcp/crawlers/web.py:188:    ) -> CrawlResult:
crawler_mcp/crawlers/web.py:386:            crawl_result = CrawlResult(
crawler_mcp/crawlers/web.py:407:            return CrawlResult(
crawler_mcp/crawlers/web.py:663:        self, request: CrawlRequest, sitemap_seeds: list[str] | None = None
crawler_mcp/crawlers/web.py:959:        self, request: CrawlRequest, sitemap_seeds: list[str]
crawler_mcp/crawlers/web.py:1044:    def _create_url_filter(self, request: CrawlRequest):
crawler_mcp/crawlers/web.py:1257:                                "Received integer %d instead of CrawlResult in streaming mode, skipping",
crawler_mcp/crawlers/web.py:1262:                        # Ensure result is a CrawlResult object
crawler_mcp/crawlers/web.py:1532:        request: CrawlRequest,
crawler_mcp/crawlers/web.py:1630:        request: CrawlRequest,
crawler_mcp/crawlers/base.py:10:from ..models.crawl import CrawlResult
crawler_mcp/crawlers/base.py:17:class BaseCrawlStrategy(ABC):
crawler_mcp/crawlers/base.py:45:    ) -> CrawlResult:
crawler_mcp/crawlers/base.py:54:            CrawlResult with crawled data
crawler_mcp/crawlers/repository.py:17:    CrawlResult,
crawler_mcp/crawlers/repository.py:54:class RepositoryCrawlStrategy(BaseCrawlStrategy):
crawler_mcp/crawlers/repository.py:101:    ) -> CrawlResult:
crawler_mcp/crawlers/repository.py:163:            return CrawlResult(
crawler_mcp/crawlers/repository.py:263:        self, result: CrawlResult, request: RepositoryRequest, clone_dir: Path
crawler_mcp/crawlers/repository.py:264:    ) -> CrawlResult:
crawler_mcp/crawlers/repository.py:276:            # (CrawlResult doesn't have metadata field)
crawler_mcp/core/orchestrator.py:20:    CrawlRequest,
crawler_mcp/core/orchestrator.py:21:    CrawlResult,
crawler_mcp/core/orchestrator.py:34:class CrawlerService(AsyncServiceBase):
crawler_mcp/core/orchestrator.py:74:        request: CrawlRequest,
crawler_mcp/core/orchestrator.py:76:    ) -> CrawlResult:
crawler_mcp/core/orchestrator.py:85:            CrawlResult with crawled pages and statistics
crawler_mcp/core/orchestrator.py:93:            return CrawlResult(
crawler_mcp/core/orchestrator.py:111:    ) -> CrawlResult:
crawler_mcp/core/orchestrator.py:122:            CrawlResult with processed files and statistics
crawler_mcp/core/orchestrator.py:137:            return CrawlResult(
crawler_mcp/core/orchestrator.py:155:    ) -> CrawlResult:
crawler_mcp/core/orchestrator.py:166:            CrawlResult with analyzed repository files and statistics
crawler_mcp/core/orchestrator.py:181:            return CrawlResult(
crawler_mcp/core/rag/processing.py:16:from ...models.crawl import CrawlResult, PageContent
crawler_mcp/core/rag/processing.py:139:        crawl_result: CrawlResult,
crawler_mcp/core/rag/processing.py:192:        from ...models.crawl import CrawlResult, CrawlStatistics, CrawlStatus
crawler_mcp/core/rag/processing.py:194:        mini_crawl_result = CrawlResult(
crawler_mcp/core/rag/processing.py:257:        self, crawl_result: CrawlResult, config: dict[str, Any]
crawler_mcp/core/rag/processing.py:341:        crawl_result: CrawlResult,
crawler_mcp/models/__init__.py:6:    CrawlRequest,
crawler_mcp/models/__init__.py:7:    CrawlResult,
crawler_mcp/models/__init__.py:23:    SourceType,
crawler_mcp/models/__init__.py:27:    "CrawlRequest",
crawler_mcp/models/__init__.py:28:    "CrawlResult",
crawler_mcp/models/__init__.py:40:    "SourceType",
crawler_mcp/models/crawl.py:24:class CrawlStatus(str, Enum):
crawler_mcp/models/crawl.py:61:class CrawlRequest(BaseModel):
crawler_mcp/models/crawl.py:115:class CrawlStatistics(BaseModel):
crawler_mcp/models/crawl.py:137:class CrawlResult(BaseModel):
crawler_mcp/models/sources.py:14:class SourceType(str, Enum):
crawler_mcp/models/sources.py:56:    source_type: SourceType
crawler_mcp/models/sources.py:156:    source_types: list[SourceType] | None = None
crawler_mcp/tools/crawling.py:16:from ..models.crawl import CrawlRequest, CrawlResult, CrawlStatus
crawler_mcp/tools/crawling.py:17:from ..models.sources import SourceType
crawler_mcp/tools/crawling.py:61:    crawl_result: CrawlResult,
crawler_mcp/tools/crawling.py:62:    source_type: SourceType,
crawler_mcp/tools/crawling.py:138:    crawl_result: CrawlResult,
crawler_mcp/tools/crawling.py:373:                    CrawlResult,
crawler_mcp/tools/crawling.py:378:                crawl_result = CrawlResult(
crawler_mcp/tools/crawling.py:392:                    SourceType.WEBPAGE,
crawler_mcp/tools/crawling.py:546:                source_type = SourceType.DIRECTORY
crawler_mcp/tools/crawling.py:585:                source_type = SourceType.REPOSITORY
crawler_mcp/tools/crawling.py:599:                request = CrawlRequest(
crawler_mcp/tools/crawling.py:667:                source_type = SourceType.WEBPAGE
crawler_mcp/core/validators.py:552:class CrawlerValidators:
crawler_mcp/core/rag/service.py:22:from ...models.crawl import CrawlResult
crawler_mcp/core/rag/service.py:457:        crawl_result: CrawlResult,
crawler_mcp/core/exceptions.py:21:class CrawlerMCPError(Exception):
crawler_mcp/core/exceptions.py:30:class CrawlError(CrawlerMCPError):
crawler_mcp/core/exceptions.py:36:class ConfigurationError(CrawlerMCPError):
crawler_mcp/core/exceptions.py:42:class ServiceError(CrawlerMCPError):
crawler_mcp/core/exceptions.py:48:class ValidationError(CrawlerMCPError):
crawler_mcp/types/crawl4ai_types.py:30:class Crawl4aiCrawlResult(Protocol):
crawler_mcp/types/crawl4ai_types.py:31:    """Protocol for crawl4ai CrawlResult."""
crawler_mcp/types/crawl4ai_types.py:45:class AsyncWebCrawler(Protocol):
crawler_mcp/types/crawl4ai_types.py:60:    ) -> Crawl4aiCrawlResult: ...
crawler_mcp/types/crawl4ai_types.py:70:    ) -> AsyncIterator[Crawl4aiCrawlResult]: ...
crawler_mcp/types/crawl4ai_types.py:74:    ) -> Crawl4aiCrawlResult | AsyncIterator[Crawl4aiCrawlResult]: ...
crawler_mcp/types/crawl4ai_types.py:85:class CrawlerRunConfig(Protocol):
crawler_mcp/types/crawl4ai_types.py:227:class DeepCrawlStrategy(Protocol):
crawler_mcp/types/crawl4ai_types.py:240:class CrawlerRunConfigAdvanced(Protocol):
crawler_mcp/types/crawl4ai_types.py:290:Crawl4aiResult = Crawl4aiCrawlResult
crawler_mcp/types/crawl4ai_types.py:300:    "Crawl4aiCrawlResult",

Server/tool registrations touching crawl:*
crawler_mcp/webhook/README.md:308:7. **Output Storage**: Save results to `/output/pr/` directory
crawler_mcp/crawlers/optimized/README.md:3:A sophisticated, production-ready web crawling system built on Crawl4AI with advanced features including GPU-aware TEI embeddings, Qdrant vector storage, adaptive concurrency control, and comprehensive content extraction capabilities.
crawler_mcp/crawlers/optimized/README.md:12:- **Vector Storage Integration**: Seamless Qdrant upsert with verification and semantic search
crawler_mcp/crawlers/optimized/README.md:89:# Qdrant Vector Storage
crawler_mcp/crawlers/optimized/README.md:213:### Qdrant Vector Storage
crawler_mcp/crawlers/optimized/README.md:307:### Vector Storage Optimization
crawler_mcp/crawlers/optimized/README.md:332:- **Cache Utilization**: Leverages Crawl4AI caching for repeat crawls
crawler_mcp/crawlers/optimized/README.md:362:  -v $(pwd)/qdrant_storage:/qdrant/storage:z \
crawler_mcp/crawlers/optimized/server.py:5:re-homes middleware and RAG tools under `crawler_mcp.crawlers.optimized`.
crawler_mcp/crawlers/optimized/server.py:30:    from ...core import EmbeddingService, RagService, VectorService
crawler_mcp/crawlers/optimized/server.py:32:except Exception:  # pragma: no cover - fallback for direct execution
crawler_mcp/crawlers/optimized/server.py:37:        RagService,
crawler_mcp/crawlers/optimized/server.py:44:    from .tools.github_pr_tools import register_github_pr_tools
crawler_mcp/crawlers/optimized/server.py:45:    from .tools.rag import register_rag_tools
crawler_mcp/crawlers/optimized/server.py:46:except Exception:  # pragma: no cover - fallback for direct execution via file path
crawler_mcp/crawlers/optimized/server.py:48:    from crawler_mcp.crawlers.optimized.tools.github_pr_tools import register_github_pr_tools  # type: ignore
crawler_mcp/crawlers/optimized/server.py:49:    from crawler_mcp.crawlers.optimized.tools.rag import register_rag_tools  # type: ignore
crawler_mcp/crawlers/optimized/server.py:100:register_github_pr_tools(mcp)
crawler_mcp/crawlers/optimized/server.py:101:register_rag_tools(mcp)
crawler_mcp/crawlers/optimized/server.py:143:        except Exception as e:  # pragma: no cover
crawler_mcp/crawlers/optimized/server.py:158:        except Exception as e:  # pragma: no cover
crawler_mcp/crawlers/optimized/server.py:161:        # RAG
crawler_mcp/crawlers/optimized/server.py:163:            async with RagService() as rag_service:
crawler_mcp/crawlers/optimized/server.py:164:                rag_health = await rag_service.health_check()
crawler_mcp/crawlers/optimized/server.py:166:                    "status": "healthy" if all(rag_health.values()) else "unhealthy",
crawler_mcp/crawlers/optimized/server.py:167:                    "component_health": rag_health,
crawler_mcp/crawlers/optimized/server.py:170:                    info["stats"] = await rag_service.get_stats()
crawler_mcp/crawlers/optimized/server.py:171:                services["rag"] = info
crawler_mcp/crawlers/optimized/server.py:172:        except Exception as e:  # pragma: no cover
crawler_mcp/crawlers/optimized/server.py:173:            services["rag"] = {"status": "error", "error": str(e)}
crawler_mcp/crawlers/optimized/server.py:176:    except Exception as e:  # pragma: no cover
crawler_mcp/crawlers/optimized/server.py:199:def _sigterm_handler(signum: int, _frame: Any) -> None:  # pragma: no cover
crawler_mcp/crawlers/optimized/server.py:242:        console.print("[dim]RAG-Enabled Web Crawling Server[/dim]")
crawler_mcp/crawlers/optimized/server.py:262:    except Exception as e:  # pragma: no cover
crawler_mcp/__init__.py:2:Crawler-MCP - FastMCP RAG-enabled web crawling MCP server
crawler_mcp/__init__.py:22:__description__ = "RAG-enabled web crawling MCP server"
crawler_mcp/crawlers/optimized/processing/url_discovery.py:878:        """Canonicalize URL for deduplication: remove fragments and normalize trailing slash."""
crawler_mcp/crawlers/optimized/processing/url_discovery.py:881:            # Remove fragment and normalize path trailing slash for directory-like paths
crawler_mcp/crawlers/optimized/processing/url_discovery.py:885:            normalized = parsed._replace(fragment="", path=path)
crawler_mcp/crawlers/optimized/processing/result_converter.py:414:            average_page_size = total_bytes / total_crawled if total_crawled > 0 else 0
crawler_mcp/crawlers/optimized/processing/result_converter.py:425:                average_page_size=average_page_size,
crawler_mcp/crawlers/optimized/processing/result_converter.py:451:            average_page_size=stats.average_content_length,
crawler_mcp/tools/__init__.py:9:    "get_rag_stats_tool",
crawler_mcp/tools/__init__.py:13:    "rag_query_tool",
crawler_mcp/core/streaming.py:12:from ..models.rag import DocumentChunk
crawler_mcp/core/streaming.py:260:    Stream process chunks with embedding and storage.
crawler_mcp/core/memory.py:189:            avg_page_size_kb: Average page size in KB
crawler_mcp/core/memory.py:216:            avg_page_size_kb: Average page size in KB
crawler_mcp/core/embeddings.py:14:from ..models.rag import EmbeddingResult
crawler_mcp/core/embeddings.py:293:                        / len(valid_texts),  # Average time per embedding
crawler_mcp/core/cache_utils.py:384:        Create a TTL cache to replace QueryCache from rag/service.py.
crawler_mcp/core/cache_utils.py:396:        Create an LRU cache to replace EmbeddingCache from rag/embedding.py.
crawler_mcp/core/caching.py:393:    Cache for RAG query results with intelligent key generation.
crawler_mcp/crawlers/directory.py:169:                average_page_size=total_bytes / len(pages) if pages else 0,
crawler_mcp/core/utils.py:52:    - Removes fragments
crawler_mcp/core/utils.py:62:        >>> normalize_url("http://example.com/path/?b=2&a=1#fragment")
crawler_mcp/core/utils.py:87:        # Reconstruct normalized URL (without fragment)
crawler_mcp/core/utils.py:95:                "",  # Remove fragment
crawler_mcp/core/rag/chunking.py:2:Text chunking strategies and token counting for RAG operations.
crawler_mcp/core/rag/chunking.py:33:def find_paragraph_boundary(search_text: str, ideal_end: int) -> int | None:
crawler_mcp/core/rag/chunking.py:34:    """Find paragraph break boundary."""
crawler_mcp/core/rag/chunking.py:35:    paragraph_breaks = [
crawler_mcp/core/rag/chunking.py:39:        b for b in paragraph_breaks if ideal_end - 100 <= b <= ideal_end + 100
crawler_mcp/core/rag/chunking.py:363:                boundary = find_paragraph_boundary(search_text, relative_ideal)
crawler_mcp/core/rag/chunking.py:543:        """Chunk text based on semantic boundaries like paragraphs and sentences."""
crawler_mcp/core/rag/chunking.py:547:        # First try to split on paragraphs
crawler_mcp/core/rag/chunking.py:548:        paragraphs = self.split_on_paragraphs(text)
crawler_mcp/core/rag/chunking.py:553:        for paragraph in paragraphs:
crawler_mcp/core/rag/chunking.py:554:            # If adding this paragraph would exceed chunk size, finalize current chunk
crawler_mcp/core/rag/chunking.py:555:            if current_chunk and len(current_chunk) + len(paragraph) > self.chunk_size:
crawler_mcp/core/rag/chunking.py:580:                    current_chunk = overlap_text + paragraph
crawler_mcp/core/rag/chunking.py:584:                    current_chunk = paragraph
crawler_mcp/core/rag/chunking.py:588:                current_chunk += paragraph
crawler_mcp/core/rag/chunking.py:621:            # Try paragraph boundary first
crawler_mcp/core/rag/chunking.py:622:            boundary = find_paragraph_boundary(search_text, relative_pos)
crawler_mcp/core/rag/chunking.py:640:    def split_on_paragraphs(self, text: str) -> list[str]:
crawler_mcp/core/rag/chunking.py:641:        """Split text on paragraph boundaries."""
crawler_mcp/core/rag/chunking.py:642:        paragraphs = text.split("\n\n")
crawler_mcp/core/rag/chunking.py:643:        return [safe_strip(p) for p in paragraphs if safe_strip(p)]
crawler_mcp/tools/crawling.py:13:from ..core import RagService
crawler_mcp/tools/crawling.py:59:async def _process_rag_if_requested(
crawler_mcp/tools/crawling.py:63:    process_with_rag: bool,
crawler_mcp/tools/crawling.py:70:    Shared RAG processing logic for all crawl types with multi-stage progress reporting.
crawler_mcp/tools/crawling.py:76:        process_with_rag: Whether to process with RAG
crawler_mcp/tools/crawling.py:83:        Dictionary with RAG processing results or error info
crawler_mcp/tools/crawling.py:85:    rag_info: dict[str, Any] = {}
crawler_mcp/tools/crawling.py:87:    if process_with_rag and crawl_result.pages:
crawler_mcp/tools/crawling.py:89:        await ctx.info(f"Starting RAG processing for {total_items} items")
crawler_mcp/tools/crawling.py:91:        # Multi-stage RAG processing with progress reporting
crawler_mcp/tools/crawling.py:93:            # Stage 1: Text chunking (0-25% of RAG progress)
crawler_mcp/tools/crawling.py:97:            # Stage 2: Generating embeddings (25-70% of RAG progress)
crawler_mcp/tools/crawling.py:101:            async with RagService() as rag_service:
crawler_mcp/tools/crawling.py:102:                rag_stats = await rag_service.process_crawl_result(
crawler_mcp/tools/crawling.py:109:                rag_info["rag_processing"] = rag_stats
crawler_mcp/tools/crawling.py:111:            # Stage 3: Vector indexing (70-90% of RAG progress)
crawler_mcp/tools/crawling.py:115:            # Stage 4: Source registration (90-100% of RAG progress)
crawler_mcp/tools/crawling.py:119:            # Source registration is now handled automatically through RAG processing
crawler_mcp/tools/crawling.py:121:            rag_info["sources_registered"] = total_items
crawler_mcp/tools/crawling.py:126:                f"RAG processing completed: {rag_stats.get('chunks_created', 0)} chunks, "
crawler_mcp/tools/crawling.py:127:                f"{rag_stats.get('embeddings_generated', 0)} embeddings"
crawler_mcp/tools/crawling.py:131:            await ctx.info(f"RAG processing failed: {e!s}")
crawler_mcp/tools/crawling.py:132:            rag_info["rag_processing_error"] = str(e)
crawler_mcp/tools/crawling.py:134:    return rag_info
crawler_mcp/tools/crawling.py:168:                "average_page_size": crawl_result.statistics.average_page_size,
crawler_mcp/tools/crawling.py:262:def register_crawling_tools(mcp: FastMCP) -> None:
crawler_mcp/tools/crawling.py:272:        process_with_rag: bool = True,
crawler_mcp/tools/crawling.py:286:            process_with_rag: Whether to process content for RAG indexing
crawler_mcp/tools/crawling.py:305:        total_steps = 5 if process_with_rag else 4
crawler_mcp/tools/crawling.py:366:            # Step 5: Process with RAG if requested
crawler_mcp/tools/crawling.py:367:            if process_with_rag:
crawler_mcp/tools/crawling.py:368:                await ctx.info("Processing content for RAG indexing")
crawler_mcp/tools/crawling.py:371:                # Create a minimal crawl result for RAG processing
crawler_mcp/tools/crawling.py:389:                rag_info = await _process_rag_if_requested(
crawler_mcp/tools/crawling.py:393:                    process_with_rag=True,
crawler_mcp/tools/crawling.py:399:                result.update(rag_info)
crawler_mcp/tools/crawling.py:421:        process_with_rag: bool = True,
crawler_mcp/tools/crawling.py:450:            process_with_rag: Whether to process content for RAG indexing
crawler_mcp/tools/crawling.py:488:            6  # Discovery, crawling, processing, validation, RAG (optional), completion
crawler_mcp/tools/crawling.py:490:        total_steps = base_steps if process_with_rag else base_steps - 1
crawler_mcp/tools/crawling.py:685:            # Step 6: Process with RAG if requested (optional step)
crawler_mcp/tools/crawling.py:686:            if process_with_rag:
crawler_mcp/tools/crawling.py:687:                await ctx.info("Processing results for RAG indexing")
crawler_mcp/tools/crawling.py:689:                rag_info = await _process_rag_if_requested(
crawler_mcp/tools/crawling.py:693:                    process_with_rag,
crawler_mcp/tools/crawling.py:699:                result.update(rag_info)
crawler_mcp/tools/crawling.py:702:            final_step = 6 if process_with_rag else 5
crawler_mcp/core/rag/processing.py:2:Main processing pipeline coordination and workflow management for RAG operations.
crawler_mcp/core/rag/processing.py:4:This module orchestrates the complete RAG processing workflow, integrating
crawler_mcp/core/rag/processing.py:5:chunking, deduplication, embedding generation, and vector storage.
crawler_mcp/core/rag/processing.py:17:from ...models.rag import DocumentChunk
crawler_mcp/core/rag/processing.py:131:    """Manages complex RAG workflow execution."""
crawler_mcp/core/rag/processing.py:144:        Execute the complete RAG processing pipeline.
crawler_mcp/core/rag/processing.py:282:                "vector_storage",
crawler_mcp/core/rag/processing.py:304:    """Main RAG processing pipeline coordinator."""
crawler_mcp/core/rag/processing.py:398:            f"Processing {total_pages} pages for RAG indexing (dedup={deduplication})"
crawler_mcp/core/rag/processing.py:657:            storage_start_time = time.monotonic()
crawler_mcp/core/rag/processing.py:663:            storage_duration = time.monotonic() - storage_start_time
crawler_mcp/core/rag/processing.py:665:                f"Stored {stored_count} document chunks in vector database in {storage_duration:.2f}s"
crawler_mcp/core/rag/processing.py:753:            batch_size: Size of storage batches
crawler_mcp/core/rag/embedding.py:2:Embedding generation pipeline with parallel processing for RAG operations.
crawler_mcp/core/rag/embedding.py:19:from ...models.rag import DocumentChunk
crawler_mcp/core/rag/embedding.py:497:        max_concurrent_storage_ops = 8  # 8 parallel storage operations
crawler_mcp/core/rag/embedding.py:501:        storage_queue: Queue[tuple[int, list[DocumentChunk]]] = Queue(
crawler_mcp/core/rag/embedding.py:502:            maxsize=max_concurrent_storage_ops * 2
crawler_mcp/core/rag/embedding.py:507:        storage_semaphore = Semaphore(max_concurrent_storage_ops)
crawler_mcp/core/rag/embedding.py:515:        # Create exactly one storage worker per chunk batch to ensure all items are processed
crawler_mcp/core/rag/embedding.py:516:        num_storage_workers = len(chunk_batches)
crawler_mcp/core/rag/embedding.py:521:            f"{num_storage_workers} storage workers"
crawler_mcp/core/rag/embedding.py:526:        storage_completed = 0
crawler_mcp/core/rag/embedding.py:545:                    # Queue batch for storage
crawler_mcp/core/rag/embedding.py:546:                    await storage_queue.put((batch_id, chunk_batch))
crawler_mcp/core/rag/embedding.py:557:                    # Still queue for storage with empty embeddings to maintain progress
crawler_mcp/core/rag/embedding.py:558:                    await storage_queue.put((batch_id, chunk_batch))
crawler_mcp/core/rag/embedding.py:560:        async def storage_worker() -> None:
crawler_mcp/core/rag/embedding.py:563:                batch_id, chunk_batch = await storage_queue.get()
crawler_mcp/core/rag/embedding.py:565:                    # Use semaphore only for the actual storage operation
crawler_mcp/core/rag/embedding.py:566:                    async with storage_semaphore:
crawler_mcp/core/rag/embedding.py:578:                    nonlocal storage_completed
crawler_mcp/core/rag/embedding.py:579:                    storage_completed += len(chunk_batch)
crawler_mcp/core/rag/embedding.py:584:                            (storage_completed / total_chunks) * 10
crawler_mcp/core/rag/embedding.py:592:                        f"Storage batch {batch_id} completed: {len(valid_chunks)} chunks stored"
crawler_mcp/core/rag/embedding.py:596:                    logger.error(f"Error in storage worker: {e}")
crawler_mcp/core/rag/embedding.py:599:                    storage_queue.task_done()
crawler_mcp/core/rag/embedding.py:607:        # Start storage workers in background
crawler_mcp/core/rag/embedding.py:608:        storage_tasks = [
crawler_mcp/core/rag/embedding.py:609:            asyncio.create_task(storage_worker()) for _ in range(num_storage_workers)
crawler_mcp/core/rag/embedding.py:616:        await storage_queue.join()
crawler_mcp/core/rag/embedding.py:618:        # Cancel storage workers (they're now idle)
crawler_mcp/core/rag/embedding.py:619:        for task in storage_tasks:
crawler_mcp/core/rag/embedding.py:623:        await asyncio.gather(*storage_tasks, return_exceptions=True)
crawler_mcp/core/rag/embedding.py:627:            f"{storage_completed} chunks processed"
crawler_mcp/crawlers/web.py:383:                average_page_size=avg_page_size,
crawler_mcp/core/rag/deduplication.py:2:Content deduplication and hash management for RAG operations.
crawler_mcp/core/rag/deduplication.py:15:from ...models.rag import DocumentChunk
crawler_mcp/core/rag/deduplication.py:225:                # Use weighted average of similarities
crawler_mcp/core/rag/deduplication.py:238:    Abstract base class for managing content deduplication across the RAG system.
crawler_mcp/models/rag.py:2:Data models for RAG (Retrieval-Augmented Generation) operations.
crawler_mcp/models/rag.py:86:class RagQuery(BaseModel):
crawler_mcp/models/rag.py:89:    """Query for RAG search operations."""
crawler_mcp/models/rag.py:129:class RagResult(BaseModel):
crawler_mcp/models/rag.py:132:    """Result of a RAG query operation."""
crawler_mcp/models/rag.py:149:    def average_score(self) -> float:
crawler_mcp/models/rag.py:150:        """Calculate average similarity score."""
crawler_mcp/core/rag/__init__.py:2:Unified RAG module providing backward compatibility and clean API access.
crawler_mcp/core/rag/__init__.py:20:    find_paragraph_boundary,
crawler_mcp/core/rag/__init__.py:41:    RagService,
crawler_mcp/core/rag/__init__.py:58:    "RagService",
crawler_mcp/core/rag/__init__.py:67:    "find_paragraph_boundary",
crawler_mcp/core/rag/__init__.py:73:def create_rag_service() -> RagService:
crawler_mcp/core/rag/__init__.py:75:    Factory function to create a RagService instance.
crawler_mcp/core/rag/__init__.py:81:        Configured RagService instance
crawler_mcp/core/rag/__init__.py:83:    return RagService()
crawler_mcp/core/rag/__init__.py:131:    Get information about the modular RAG structure.
crawler_mcp/core/rag/__init__.py:146:# This ensures existing code like `from crawler_mcp.core.rag import RagService` still works
crawler_mcp/models/__init__.py:12:from .rag import (
crawler_mcp/models/__init__.py:15:    RagQuery,
crawler_mcp/models/__init__.py:16:    RagResult,
crawler_mcp/models/__init__.py:34:    "RagQuery",
crawler_mcp/models/__init__.py:35:    "RagResult",
crawler_mcp/models/crawl.py:123:    average_page_size: float = 0.0
crawler_mcp/models/sources.py:101:        """Calculate average chunk size."""
crawler_mcp/crawlers/optimized/AGENTS.md:16:- Full tests + coverage: `uv run pytest --cov=crawler_mcp --cov-report=term-missing`
crawler_mcp/crawlers/optimized/AGENTS.md:34:- PRs include: summary, rationale, linked issues, test coverage notes, screenshots/logs when relevant.
crawler_mcp/crawlers/optimized/tools/rag.py:2:FastMCP tools for RAG operations (optimized namespace).
crawler_mcp/crawlers/optimized/tools/rag.py:13:from ....core import RagService, VectorService
crawler_mcp/crawlers/optimized/tools/rag.py:14:from ....models.rag import RagQuery
crawler_mcp/crawlers/optimized/tools/rag.py:20:def register_rag_tools(mcp: FastMCP) -> None:
crawler_mcp/crawlers/optimized/tools/rag.py:21:    """Register all RAG tools with the FastMCP server."""
crawler_mcp/crawlers/optimized/tools/rag.py:24:    async def rag_query(
crawler_mcp/crawlers/optimized/tools/rag.py:33:        await ctx.info(f"Performing RAG query: '{query}' (limit: {limit})")
crawler_mcp/crawlers/optimized/tools/rag.py:42:        progress_tracker = progress_middleware.create_tracker(f"rag_{hash(query)}")
crawler_mcp/crawlers/optimized/tools/rag.py:45:            rag_query_obj = RagQuery(
crawler_mcp/crawlers/optimized/tools/rag.py:55:            async with RagService() as rag_service:
crawler_mcp/crawlers/optimized/tools/rag.py:57:                rag_result = await rag_service.query(rag_query_obj, rerank=rerank)
crawler_mcp/crawlers/optimized/tools/rag.py:62:                "query": rag_result.query,
crawler_mcp/crawlers/optimized/tools/rag.py:63:                "total_matches": rag_result.total_matches,
crawler_mcp/crawlers/optimized/tools/rag.py:66:                    "total_time": rag_result.processing_time,
crawler_mcp/crawlers/optimized/tools/rag.py:67:                    "embedding_time": rag_result.embedding_time,
crawler_mcp/crawlers/optimized/tools/rag.py:68:                    "search_time": rag_result.search_time,
crawler_mcp/crawlers/optimized/tools/rag.py:69:                    "rerank_time": rag_result.rerank_time,
crawler_mcp/crawlers/optimized/tools/rag.py:72:                    "average_score": rag_result.average_score,
crawler_mcp/crawlers/optimized/tools/rag.py:73:                    "best_match_score": rag_result.best_match_score,
crawler_mcp/crawlers/optimized/tools/rag.py:74:                    "high_confidence_matches": rag_result.has_high_confidence_matches,
crawler_mcp/crawlers/optimized/tools/rag.py:76:                "timestamp": rag_result.timestamp.isoformat(),
crawler_mcp/crawlers/optimized/tools/rag.py:79:            for match in rag_result.matches:
crawler_mcp/crawlers/optimized/tools/rag.py:101:                f"RAG query completed: {rag_result.total_matches} matches in {rag_result.processing_time:.3f}s "
crawler_mcp/crawlers/optimized/tools/rag.py:102:                f"(avg score: {rag_result.average_score:.3f})"
crawler_mcp/crawlers/optimized/tools/rag.py:106:            error_msg = f"RAG query failed: {e!s}"
crawler_mcp/crawlers/optimized/tools/github_pr_tools.py:187:def register_github_pr_tools(mcp: FastMCP) -> None:
crawler_mcp/core/vectors/statistics.py:62:                    "average_chunk_size": "unknown (lightweight mode)",
crawler_mcp/core/vectors/statistics.py:72:                "average_chunk_size": 0.0,
crawler_mcp/core/vectors/statistics.py:111:            # Calculate average chunk size
crawler_mcp/core/vectors/statistics.py:113:                stats["average_chunk_size"] = (
crawler_mcp/core/vectors/statistics.py:393:                "average_docs_per_source": (
crawler_mcp/core/vectors/statistics.py:397:                "average_chunk_size": stats.get("average_chunk_size", 0.0),
crawler_mcp/core/vectors/statistics.py:413:        # - Average vector magnitude
crawler_mcp/crawlers/optimized/shared/reporting.py:68:    avg_h = _hdr("X-Average-Page-Size-Human", "-")
crawler_mcp/crawlers/optimized/shared/reporting.py:105:    except Exception:  # pragma: no cover
crawler_mcp/crawlers/optimized/shared/reporting.py:265:        avg_cpu = sysm.get("average_cpu_usage")
crawler_mcp/core/vectors/operations.py:17:from ...models.rag import DocumentChunk
crawler_mcp/crawlers/optimized/middleware/logging.py:17:    async def __call__(self, request: Request) -> Response:  # pragma: no cover - used by transport
crawler_mcp/crawlers/optimized/core/adaptive_dispatcher.py:76:        avg_cpu = metrics.average_cpu_usage or 0.0
crawler_mcp/crawlers/optimized/middleware/error.py:17:    async def __call__(self, request: Request) -> Response:  # pragma: no cover - used by transport
crawler_mcp/core/__init__.py:8:from .rag import RagService
crawler_mcp/core/__init__.py:13:    "RagService",
crawler_mcp/crawlers/optimized/middleware/progress.py:60:    async def __call__(self, request: Request) -> Response:  # pragma: no cover - used by transport
crawler_mcp/crawlers/optimized/middleware/progress.py:78:async def _dummy_app(_request: Request) -> Response:  # pragma: no cover
crawler_mcp/core/vectors/__init__.py:12:from ...models.rag import DocumentChunk, SearchMatch
crawler_mcp/core/vectors/search.py:20:from ...models.rag import DocumentChunk, SearchMatch
crawler_mcp/core/rag/service.py:2:Service orchestration and query processing for RAG operations.
crawler_mcp/core/rag/service.py:4:This module provides the main RagService class that coordinates all RAG operations
crawler_mcp/core/rag/service.py:23:from ...models.rag import DocumentChunk, RagQuery, RagResult, SearchMatch
crawler_mcp/core/rag/service.py:34:    Simple in-memory cache for RAG query results with TTL support.
crawler_mcp/core/rag/service.py:38:        self.cache: dict[str, tuple[RagResult, datetime]] = {}
crawler_mcp/core/rag/service.py:78:    ) -> RagResult | None:
crawler_mcp/core/rag/service.py:111:        result: RagResult,
crawler_mcp/core/rag/service.py:210:            "average_query_time": avg_query_time,
crawler_mcp/core/rag/service.py:218:class RagService:
crawler_mcp/core/rag/service.py:220:    Service for RAG operations combining embedding generation and vector search.
crawler_mcp/core/rag/service.py:230:    def __new__(cls) -> "RagService":
crawler_mcp/core/rag/service.py:303:        if RagService._lock is None:
crawler_mcp/core/rag/service.py:304:            RagService._lock = asyncio.Lock()
crawler_mcp/core/rag/service.py:307:        RagService._initialized = True
crawler_mcp/core/rag/service.py:309:    async def __aenter__(self) -> "RagService":
crawler_mcp/core/rag/service.py:311:        if RagService._lock is None:
crawler_mcp/core/rag/service.py:312:            RagService._lock = asyncio.Lock()
crawler_mcp/core/rag/service.py:313:        async with RagService._lock:
crawler_mcp/core/rag/service.py:331:        if RagService._lock is None:
crawler_mcp/core/rag/service.py:332:            RagService._lock = asyncio.Lock()
crawler_mcp/core/rag/service.py:333:        async with RagService._lock:
crawler_mcp/core/rag/service.py:340:                RagService._auto_opened = False
crawler_mcp/core/rag/service.py:366:        if RagService._lock is None:
crawler_mcp/core/rag/service.py:367:            RagService._lock = asyncio.Lock()
crawler_mcp/core/rag/service.py:368:        async with RagService._lock:
crawler_mcp/core/rag/service.py:374:                RagService._auto_opened = True
crawler_mcp/core/rag/service.py:390:    def _find_paragraph_boundary(self, search_text: str, ideal_end: int) -> int | None:
crawler_mcp/core/rag/service.py:391:        """Find paragraph break boundary."""
crawler_mcp/core/rag/service.py:392:        from .chunking import find_paragraph_boundary
crawler_mcp/core/rag/service.py:394:        return find_paragraph_boundary(search_text, ideal_end)
crawler_mcp/core/rag/service.py:487:    async def query(self, query: RagQuery, rerank: bool = True) -> RagResult:
crawler_mcp/core/rag/service.py:489:        Perform a RAG query to find relevant documents.
crawler_mcp/core/rag/service.py:492:            query: The RAG query parameters
crawler_mcp/core/rag/service.py:496:            RagResult with matched documents
crawler_mcp/core/rag/service.py:584:            result = RagResult(
crawler_mcp/core/rag/service.py:595:                f"RAG query completed: {len(filtered_matches)} matches in {processing_time:.3f}s "
crawler_mcp/core/rag/service.py:623:            logger.error(f"Error processing RAG query: {e}")
crawler_mcp/core/rag/service.py:624:            raise ToolError(f"RAG query failed: {e!s}") from e
crawler_mcp/core/rag/service.py:769:                # Combine scores (weighted average)
crawler_mcp/core/rag/service.py:803:        Get RAG system statistics.
crawler_mcp/core/rag/service.py:844:            logger.error(f"Error getting RAG stats: {e}")
crawler_mcp/crawlers/optimized/factories/browser_factory.py:75:            storage_state=None,  # No persistent storage
crawler_mcp/crawlers/optimized/utils/monitoring.py:39:    average_content_length: float = 0.0
crawler_mcp/crawlers/optimized/utils/monitoring.py:45:    average_cpu_usage: float = 0.0
crawler_mcp/crawlers/optimized/utils/monitoring.py:46:    average_process_cpu_usage: float = 0.0
crawler_mcp/crawlers/optimized/utils/monitoring.py:253:        # Calculate average content length
crawler_mcp/crawlers/optimized/utils/monitoring.py:254:        self.metrics.average_content_length = (
crawler_mcp/crawlers/optimized/utils/monitoring.py:361:            # Running average of quality scores
crawler_mcp/crawlers/optimized/utils/monitoring.py:483:            # Average CPU over recent samples
crawler_mcp/crawlers/optimized/utils/monitoring.py:493:                    self.metrics.average_cpu_usage = total / count
crawler_mcp/crawlers/optimized/utils/monitoring.py:494:                    self.metrics.average_process_cpu_usage = total_proc / count
crawler_mcp/crawlers/optimized/utils/monitoring.py:582:                "average_page_size_human": _fmt_bytes(avg_size),
crawler_mcp/crawlers/optimized/utils/monitoring.py:599:                "average_content_length": self.metrics.average_content_length,
crawler_mcp/crawlers/optimized/utils/monitoring.py:600:                "average_content_human": _fmt_bytes(
crawler_mcp/crawlers/optimized/utils/monitoring.py:601:                    self.metrics.average_content_length
crawler_mcp/crawlers/optimized/utils/monitoring.py:625:                "average_cpu_usage": self.metrics.average_cpu_usage,
crawler_mcp/crawlers/optimized/utils/monitoring.py:626:                "process_cpu_avg": self.metrics.average_process_cpu_usage,
crawler_mcp/crawlers/optimized/cli/pr.py:41:from ..tools.github_pr_tools import _apply_filters, list_pr_items_impl
crawler_mcp/crawlers/optimized/core/strategy.py:82:        self.logger.info(f"Qdrant storage configured: {self.config.enable_qdrant}")
crawler_mcp/crawlers/optimized/core/strategy.py:98:                "Qdrant configured but embeddings disabled - no vector storage will occur"
crawler_mcp/crawlers/optimized/core/strategy.py:1184:                    f"via TEI ({self.config.tei_endpoint}) for Qdrant storage"
crawler_mcp/crawlers/optimized/core/strategy.py:1189:                    "Embeddings disabled for GitHub PR: Qdrant vector storage not configured. "
crawler_mcp/crawlers/optimized/core/strategy.py:1192:        except Exception as e:  # pragma: no cover
crawler_mcp/crawlers/optimized/core/strategy.py:1284:                        f"via TEI ({self.config.tei_endpoint}) for Qdrant storage"
crawler_mcp/crawlers/optimized/core/strategy.py:1289:                        "Embeddings disabled: Qdrant vector storage not configured. "
crawler_mcp/crawlers/optimized/core/strategy.py:1298:            except Exception as e:  # pragma: no cover
crawler_mcp/crawlers/optimized/core/strategy.py:1403:                response.response_headers["X-Average-Page-Size-Human"] = _fmt_bytes(
crawler_mcp/crawlers/optimized/core/strategy.py:1566:                # Report average batch size used
crawler_mcp/crawlers/optimized/core/parallel_engine.py:37:    average_content_length: float
crawler_mcp/crawlers/optimized/core/parallel_engine.py:751:        # Suspicious average word length
crawler_mcp/crawlers/optimized/core/parallel_engine.py:807:            average_content_length=avg_content_length,
crawler_mcp/crawlers/optimized/cli/crawl.py:332:def _pick_paragraph(text: str) -> str:
crawler_mcp/crawlers/optimized/cli/crawl.py:618:                para = _pick_paragraph(str(payload.get("text", "")))
crawler_mcp/crawlers/optimized/cli/crawl.py:650:            para = _pick_paragraph(str(payload.get("text", "")))
