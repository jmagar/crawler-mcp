[DEBUG] CLI: Delegating hierarchical memory load to server for CWD: /home/jmagar/code/crawl-mcp (memoryImportFormat: tree)
[DEBUG] [MemoryDiscovery] Loading server hierarchical memory for CWD: /home/jmagar/code/crawl-mcp (importFormat: tree)
[DEBUG] [MemoryDiscovery] Searching for GEMINI.md starting from CWD: /home/jmagar/code/crawl-mcp
[DEBUG] [MemoryDiscovery] Determined project root: /home/jmagar/code/crawl-mcp
[DEBUG] [BfsFileSearch] Scanning [1/200]: batch of 1
[DEBUG] [BfsFileSearch] Scanning [14/200]: batch of 13
[DEBUG] [BfsFileSearch] Scanning [29/200]: batch of 15
[DEBUG] [BfsFileSearch] Scanning [44/200]: batch of 15
[DEBUG] [BfsFileSearch] Scanning [59/200]: batch of 15
[DEBUG] [BfsFileSearch] Scanning [74/200]: batch of 15
[DEBUG] [BfsFileSearch] Scanning [89/200]: batch of 15
[DEBUG] [BfsFileSearch] Scanning [104/200]: batch of 15
[DEBUG] [BfsFileSearch] Scanning [119/200]: batch of 15
[DEBUG] [BfsFileSearch] Scanning [134/200]: batch of 15
[DEBUG] [BfsFileSearch] Scanning [149/200]: batch of 15
[DEBUG] [BfsFileSearch] Scanning [164/200]: batch of 15
[DEBUG] [BfsFileSearch] Scanning [179/200]: batch of 15
[DEBUG] [BfsFileSearch] Scanning [194/200]: batch of 15
[DEBUG] [BfsFileSearch] Scanning [200/200]: batch of 6
[DEBUG] [MemoryDiscovery] Final ordered GEMINI.md paths to read: []
[DEBUG] [MemoryDiscovery] No GEMINI.md files found in hierarchy of the workspace.
Flushing log events to Clearcut.
Data collection is disabled.
Of course. Here is a detailed, implementation-ready plan for introducing the `scrape` and `crawl` tools into the `optimized` namespace.

### **Phase 1: Foundational Components & `scrape` Tool**

This phase establishes the core structure and the simpler single-page extraction tool.

**1. File Layout & Initial Setup**

*   **New Tool File:** `crawler_mcp/crawlers/optimized/tools/crawling.py`
    *   This file will contain the `scrape` tool, the `crawl` tool, and the registration function.
*   **New Strategy File:** `crawler_mcp/crawlers/optimized/core/crawl_strategies.py`
    *   Will contain the logic for different crawl types (Website, Directory, etc.).
*   **Model Updates:** `crawler_mcp/models/crawl.py`
    *   Add `ScrapeResult` and `CrawlResult` Pydantic models.
*   **Server Registration:** `crawler_mcp/crawlers/optimized/server.py`
    *   Update to import and call the new registration function.

**2. `scrape` Tool: API & Implementation Plan**

*   **File:** `crawler_mcp/crawlers/optimized/tools/crawling.py`
*   **Signature:**
    ```python
    from crawler_mcp.core.exceptions import ToolError
    from crawler_mcp.models.crawl import ScrapeResult
    from crawler_mcp.crawlers.optimized.config import OptimizedConfig
    # ... other imports

    async def scrape(
        ctx: ToolContext,
        url: str,
        rag_ingest: bool = False,
        screenshot: bool = False
    ) -> ScrapeResult:
        """
        Extracts intelligent content from a single URL using the optimized stack.
        """
        # ... implementation ...
    ```
*   **Parameters & Validation:**
    *   `url: str`: Must be a valid URL (starts with `http://` or `https://`). Raise `ToolError` if invalid.
    *   `rag_ingest: bool = False`: If `True`, extracted content will be passed to the RAG service.
    *   `screenshot: bool = False`: If `True`, a screenshot of the page is saved.
*   **Result Schema:**
    *   **File:** `crawler_mcp/models/crawl.py`
    ```python
    class ScrapeResult(BaseModel):
        preview: str = Field(description="A 100-word preview of the extracted content.")
        url: str
        title: str | None
        language: str | None
        word_count: int
        content_path: str = Field(description="Path to the file with the full extracted content.")
        screenshot_path: str | None = Field(description="Path to the saved screenshot, if requested.")
        manifest_path: str = Field(description="Path to the JSON manifest with full scrape details.")
    ```
*   **Progress Phase Mapping (`ctx.report_progress`):**
    1.  `"initializing"`: Validating parameters.
    2.  `"fetching"`: Acquiring browser and navigating to URL.
    3.  `"extracting"`: Running AI-aware content extraction.
    4.  `"saving"`: Writing content, manifest, and optional screenshot via `OutputManager`.
    5.  `"completing"`: Finalizing and preparing the result.

### **Phase 2: `crawl` Tool & Strategy-Based Architecture**

This phase implements the unified, multi-type `crawl` tool.

**1. `crawl` Tool: API & Dispatcher Logic**

*   **File:** `crawler_mcp/crawlers/optimized/tools/crawling.py`
*   **Signature:**
    ```python
    from crawler_mcp.models.crawl import CrawlResult

    async def crawl(
        ctx: ToolContext,
        target: str,
        rag_ingest: bool = False,
        crawl_depth: int = 2,
        limit: int = 100
    ) -> CrawlResult:
        """
        Auto-detects input type and crawls websites, repositories, or directories.
        """
        # 1. Input detection logic
        # 2. Select and instantiate appropriate strategy
        # 3. Execute strategy and return result
    ```
*   **Parameters & Validation:**
    *   `target: str`: The resource to crawl.
    *   `crawl_depth: int`: Clamp between `0` and `5`. Default `2`.
    *   `limit: int`: Clamp between `1` and `1000`. Default `100`.
*   **Input Detection Logic (in `crawl` function):**
    *   If `target` matches a GitHub PR URL format -> `GitHubPRCrawlStrategy`.
    *   If `target` starts with `http` -> `WebsiteCrawlStrategy`.
    *   If `os.path.isdir(target)` -> `DirectoryCrawlStrategy`.
    *   If `target` ends with `.git` or is a valid git SSH path -> `RepositoryCrawlStrategy`.
    *   Else, raise `ToolError("Unsupported target type.")`.

**2. Crawl Strategies**

*   **File:** `crawler_mcp/crawlers/optimized/core/crawl_strategies.py`
*   **Base Class:**
    ```python
    class BaseCrawlStrategy(ABC):
        def __init__(self, ctx: ToolContext, config: OptimizedConfig, ...): ...
        @abstractmethod
        async def discover_items(self) -> list[Source]: ...
        @abstractmethod
        async def process_item(self, item: Source) -> CrawlItem: ...
        async def execute(self) -> CrawlResult:
            # Uses ParallelEngine to run process_item over discovered items
            ...
    ```
*   **Implementations:** `WebsiteCrawlStrategy`, `DirectoryCrawlStrategy`, `RepositoryCrawlStrategy`, `GitHubPRCrawlStrategy`. The `GitHubPRCrawlStrategy` will be a thin wrapper around the existing `github_pr_tools`.

**3. Result Schema**

*   **File:** `crawler_mcp/models/crawl.py`
    ```python
    from crawler_mcp.models.sources import Source

    class CrawlItem(BaseModel):
        source: Source
        status: Literal["success", "failed"]
        error_message: str | None = None
        content_path: str | None

    class CrawlResult(BaseModel):
        kind: Literal["website", "directory", "repository", "github_pr"]
        target: str
        total_items: int
        successful: int
        failed: int
        items_preview: list[CrawlItem] = Field(description="Preview of the first 10 items processed.")
        manifest_path: str = Field(description="Path to the JSON manifest with all crawl items.")
    ```

**4. Progress Phase Mapping (`ctx.report_progress`):**
    1.  `"initializing"`: Input detection and strategy selection.
    2.  `"discovering"`: Running the `discover_items` method of the strategy.
    3.  `"crawling"`: Processing items with `ParallelEngine`. Progress reported as `(processed, total)`.
    4.  `"saving"`: Writing the final manifest via `OutputManager`.
    5.  `"completing"`: Finalizing.

### **Phase 3: Registration, Testing & Cutover**

**1. Tool Registration**

*   **File:** `crawler_mcp/crawlers/optimized/tools/crawling.py`
    ```python
    def register_crawling_tools(mcp: FastMCP) -> None:
        mcp.register_tool(scrape)
        mcp.register_tool(crawl)
    ```
*   **File:** `crawler_mcp/crawlers/optimized/server.py`
    *   Add the following lines before the server is initialized:
    ```python
    from crawler_mcp.crawlers.optimized.tools.crawling import register_crawling_tools
    # ...
    register_crawling_tools(mcp)
    ```

**2. Import Map (Core Dependencies)**

The new tools will primarily import from:
*   `crawler_mcp.crawlers.optimized.config`: `OptimizedConfig`
*   `crawler_mcp.crawlers.optimized.core.parallel_engine`: `ParallelEngine`
*   `crawler_mcp.crawlers.optimized.core.strategy`: `Strategy`
*   `crawler_mcp.crawlers.optimized.factories`: `BrowserFactory`, `ContentExtractor`
*   `crawler_mcp.crawlers.optimized.tools`: `github_pr_tools`
*   `crawler_mcp.crawlers.optimized.utils`: `OutputManager`, `LogManager`
*   `crawler_mcp.core.exceptions`: `ToolError`
*   `crawler_mcp.models`: `crawl`, `rag`, `sources`

**3. Error Handling Approach**

*   Use specific `try...except` blocks for I/O, network, and browser errors.
*   All user-facing errors must be wrapped in `ToolError` with a clear, concise message.
*   The `ParallelEngine` will be configured to continue on task failure, and individual item failures will be logged and aggregated in the final `CrawlResult`.

**4. Test Plan**

*   **Unit Tests:**
    *   Test `scrape` parameter validation and error handling (mocked dependencies).
    *   Test `crawl` input detection logic to ensure correct strategy selection.
    *   Test each `BaseCrawlStrategy` subclass in isolation, mocking I/O and network calls.
*   **Integration Tests:**
    *   `scrape`: Test against a live (or locally served) web page. Verify output files.
    *   `crawl`:
        *   Run against a small local website (2 levels deep).
        *   Run against a test directory with various file types.
        *   Run against a test GitHub PR URL.
    *   Verify that `rag_ingest=True` correctly triggers the RAG service call (mocked).
    *   Verify progress reporting phases are accurate for each tool.

**5. Cutover Checklist**

1.  [ ] Implement and merge `scrape` and `crawl` tools according to the plan.
2.  [ ] Achieve full unit and integration test coverage.
3.  [ ] Update `crawler_mcp/crawlers/optimized/cli/crawl.py` to use the new `crawl` tool via an API call to the server, deprecating its direct implementation.
4.  [ ] Add a `DeprecationWarning` to the legacy `crawler_mcp/tools/crawling.py`.
5.  [ ] Update all relevant documentation (`AGENTS.md`, `CRAWL4AI_CONFIG.md`) to reflect the new tools and deprecate the old ones.
6.  [ ] After a stabilization period, remove the legacy `crawling.py` tool and its registration.
