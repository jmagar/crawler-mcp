[DEBUG] CLI: Delegating hierarchical memory load to server for CWD: /home/jmagar/code/crawl-mcp (memoryImportFormat: tree)
[DEBUG] [MemoryDiscovery] Loading server hierarchical memory for CWD: /home/jmagar/code/crawl-mcp (importFormat: tree)
[DEBUG] [MemoryDiscovery] Searching for GEMINI.md starting from CWD: /home/jmagar/code/crawl-mcp
[DEBUG] [MemoryDiscovery] Determined project root: /home/jmagar/code/crawl-mcp
[DEBUG] [BfsFileSearch] Scanning [1/200]: batch of 1
[DEBUG] [BfsFileSearch] Scanning [14/200]: batch of 13
[DEBUG] [BfsFileSearch] Scanning [29/200]: batch of 15
[DEBUG] [BfsFileSearch] Scanning [44/200]: batch of 15
[DEBUG] [BfsFileSearch] Scanning [59/200]: batch of 15
[DEBUG] [BfsFileSearch] Scanning [74/200]: batch of 15
[DEBUG] [BfsFileSearch] Scanning [89/200]: batch of 15
[DEBUG] [BfsFileSearch] Scanning [104/200]: batch of 15
[DEBUG] [BfsFileSearch] Scanning [119/200]: batch of 15
[DEBUG] [BfsFileSearch] Scanning [134/200]: batch of 15
[DEBUG] [BfsFileSearch] Scanning [149/200]: batch of 15
[DEBUG] [BfsFileSearch] Scanning [164/200]: batch of 15
[DEBUG] [BfsFileSearch] Scanning [179/200]: batch of 15
[DEBUG] [BfsFileSearch] Scanning [194/200]: batch of 15
[DEBUG] [BfsFileSearch] Scanning [200/200]: batch of 6
[DEBUG] [MemoryDiscovery] Final ordered GEMINI.md paths to read: []
[DEBUG] [MemoryDiscovery] No GEMINI.md files found in hierarchy of the workspace.
Flushing log events to Clearcut.
Data collection is disabled.
Here is a phased implementation plan for introducing first-class crawling tools into the MCP server.

### **Phase 1: Foundational Tool & Core Integration**

This phase focuses on creating the new tool module, implementing the most complex crawler (`crawl_website`), and ensuring it correctly integrates with the optimized core components.

**Rationale:** By starting with the web crawler, we tackle the most significant architectural challenges first (browser interaction, async discovery, parallel execution). This de-risks the project early.

**1.1. File Layout**

*   **Create New File:** `crawler_mcp/crawlers/optimized/tools/crawling.py`
    *   This will house the tool definitions and the registration function.
*   **Modify Existing File:** `crawler_mcp/crawlers/optimized/server.py`
    *   To import and call the new registration function.

**1.2. API Signature & Implementation Sketch (`crawling.py`)**

A registration function will manage adding tools to the MCP instance. The tools themselves will be `async` methods that orchestrate the crawl.

```python
# crawler_mcp/crawlers/optimized/tools/crawling.py
from typing import Any, Dict, List, Optional
from crawler_mcp.crawlers.optimized.core.parallel_engine import ParallelEngine
from crawler_mcp.crawlers.optimized.core.strategy import WebsiteCrawlStrategy
from crawler_mcp.crawlers.optimized.config import OptimizedConfig, CrawlType
from crawler_mcp.crawlers.optimized.utils.output_manager import OutputManager
from crawler_mcp.crawlers.optimized.utils.log_manager import LogManager
# ... other necessary imports (FastMCP, ToolContext, ToolError, RagService)

# Placeholder for RAG service, to be fully implemented in Phase 3
# from crawler_mcp.core.rag.service import RagService

async def _execute_crawl(config: OptimizedConfig, ctx: ToolContext) -> Dict[str, Any]:
    """Shared crawl execution logic."""
    # 1. Setup: Initialize OutputManager, LogManager from config
    output_manager = OutputManager(config)
    log_manager = LogManager(config.log_directory)
    logger = log_manager.get_logger()
    logger.info(f"Starting crawl for job: {config.job_name}")

    # 2. Strategy Selection: Choose strategy based on config.crawl_type
    #    For now, we only implement WebsiteCrawlStrategy
    strategy = WebsiteCrawlStrategy(config, output_manager, logger, ctx.report_progress)

    # 3. Execution: Run the ParallelEngine
    engine = ParallelEngine(config, strategy, logger)
    crawl_results = await engine.run()

    # 4. Summary: Generate a summary from results
    summary = {
        "status": "completed",
        "total_crawled": len(crawl_results),
        "output_path": str(output_manager.get_crawl_output_dir()),
        "log_path": str(log_manager.log_file_path),
        # ... other stats like errors, content types, etc.
    }
    return summary

def register_crawling_tools(mcp: FastMCP) -> None:
    """Registers the new crawling tools with the MCP server."""

    @mcp.tool(
        name="optimized_crawl_website",
        description="Crawls a website from a seed URL using the optimized engine."
    )
    async def crawl_website(
        ctx: ToolContext,
        url: str,
        max_depth: int = 2,
        max_links: int = 100,
        include_pdfs: bool = True,
        ingest_to_rag: bool = False,
        rag_collection_name: Optional[str] = None
    ) -> Dict[str, Any]:
        """
        Crawls a website, extracts content, and saves it locally.

        Args:
            ctx: The tool context for progress reporting.
            url: The initial URL to start crawling from.
            max_depth: Maximum depth of links to follow.
            max_links: Maximum number of pages to crawl.
            include_pdfs: Whether to download and extract text from PDF files.
            ingest_to_rag: If True, automatically ingest crawled content into the RAG service.
            rag_collection_name: The name of the RAG collection to ingest into.
        """
        # 1. Input Validation
        if not url.startswith(('http://', 'https://')):
            raise ToolError("Invalid URL. Must start with http:// or https://")

        # 2. Configuration Mapping
        config = OptimizedConfig(
            urls=[url],
            crawl_type=CrawlType.WEBSITE,
            max_depth=max_depth,
            max_total_links=max_links,
            pdf_extraction=include_pdfs,
            # ... map other params ...
        )

        # 3. Execute Crawl
        summary = await _execute_crawl(config, ctx)

        # 4. Optional RAG Ingestion (Logic to be added in Phase 3)
        if ingest_to_rag:
            ctx.report_progress("Starting RAG ingestion...")
            # rag_service = RagService()
            # await rag_service.ingest_from_directory(summary["output_path"], collection_name=rag_collection_name)
            summary["rag_ingestion_status"] = "completed" # or "skipped" if no collection name

        return summary

# 1.3. Server Integration (`server.py`)

```python
# crawler_mcp/crawlers/optimized/server.py

# ... existing imports
from crawler_mcp.crawlers.optimized.tools.github_pr_tools import register_github_pr_tools
from crawler_mcp.crawlers.optimized.tools.rag import register_rag_tools
from crawler_mcp.crawlers.optimized.tools.crawling import register_crawling_tools # ADD THIS

# ...
def setup_mcp_server() -> FastMCP:
    # ...
    register_github_pr_tools(mcp)
    register_rag_tools(mcp)
    register_crawling_tools(mcp) # ADD THIS
    # ...
    return mcp
```

### **Phase 2: Repository and Directory Crawlers**

**Rationale:** Extend the pattern established in Phase 1 to the simpler file-based crawlers.

**2.1. API Signatures & Implementation (`crawling.py`)**

Add the following tool definitions inside `register_crawling_tools`. They will use the same `_execute_crawl` helper but with different strategies and configurations.

```python
# crawler_mcp/crawlers/optimized/tools/crawling.py (inside register_crawling_tools)

@mcp.tool(
    name="optimized_crawl_repository",
    description="Crawls a Git repository using the optimized engine."
)
async def crawl_repository(
    ctx: ToolContext,
    repo_url: str,
    branch: Optional[str] = None,
    ingest_to_rag: bool = False,
    rag_collection_name: Optional[str] = None
) -> Dict[str, Any]:
    # 1. Validation: Check for valid Git URL.
    # 2. Config: Create OptimizedConfig with CrawlType.REPOSITORY, repo_url, branch.
    # 3. Execute: Call _execute_crawl(config, ctx).
    # 4. RAG Ingestion (Phase 3).
    # 5. Return summary.
    ...

@mcp.tool(
    name="optimized_crawl_directory",
    description="Crawls a local directory path using the optimized engine."
)
async def crawl_directory(
    ctx: ToolContext,
    path: str,
    glob_pattern: str = "**/*",
    ingest_to_rag: bool = False,
    rag_collection_name: Optional[str] = None
) -> Dict[str, Any]:
    # 1. Validation: Check if path exists and is a directory.
    # 2. Config: Create OptimizedConfig with CrawlType.DIRECTORY, path, glob_pattern.
    # 3. Execute: Call _execute_crawl(config, ctx).
    # 4. RAG Ingestion (Phase 3).
    # 5. Return summary.
    ...
```

**2.2. Core Logic (`strategy.py`)**

New strategies will be needed to support these crawl types.

*   **Create:** `RepositoryCrawlStrategy` and `DirectoryCrawlStrategy` in `crawler_mcp/crawlers/optimized/core/strategy.py`.
*   **Logic:** These strategies will not use the browser. They will use file system operations (`os.walk`, `glob`) or Git libraries (`git.Repo`) to discover files and will simply read their content.

### **Phase 3: RAG Integration and Finalization**

**Rationale:** Complete the feature set by enabling the optional, decoupled RAG ingestion.

**3.1. Implement RAG Logic**

In `crawling.py`, fully implement the `ingest_to_rag` block within each tool. This involves:
1.  Instantiating `RagService`.
2.  Calling a method like `ingest_from_directory(path, collection_name)`.
3.  Adding robust error handling and progress reporting for the ingestion step.

### **Structured Result Schema**

All tools will return a JSON object (dictionary) adhering to this schema:

```json
{
  "status": "completed" | "failed",
  "total_crawled": 125,
  "errors": 3,
  "output_path": "/path/to/output/crawls/example.com/20250903_103000",
  "log_path": "/path/to/output/logs/example.com/20250903_103000.log",
  "rag_ingestion_status": "completed" | "skipped" | "failed",
  "rag_collection_name": "my_collection",
  "summary_stats": {
    "html": 110,
    "pdf": 5,
    "text": 10
  }
}
```

### **Test Plan**

*   **Location:** `crawler_mcp/crawlers/optimized/tests/test_crawling_tools.py`
*   **Unit Tests:**
    *   Test input validation for each tool (e.g., invalid URLs, non-existent paths).
    *   Test `OptimizedConfig` mapping from tool parameters.
    *   Mock `_execute_crawl` and verify it's called with the correct config.
*   **Integration Tests (Async):**
    *   `test_crawl_website_e2e`: Crawl a small, local mock HTTP server (2-3 pages). Verify that output files are created and the summary is correct.
    *   `test_crawl_directory_e2e`: Crawl a temporary directory with a few text files. Verify outputs.
    *   `test_rag_ingestion_flag`: Run a crawl with `ingest_to_rag=True`. Mock `RagService` and assert that its `ingest_from_directory` method is called with the correct path and collection name.

### **Validation & Rollout**

1.  **Local Validation:** Run each tool via an MCP client against local and public targets.
    *   Verify output directory structure matches `output/crawls/{job_name}/{timestamp}`.
    *   Check log files for correct formatting and progress messages.
    *   Confirm performance is reasonable for small-scale crawls.
2.  **Documentation:**
    *   Update `AGENTS.md` with example invocations for each new tool.
    *   Ensure `README.md` in the `optimized` directory reflects the new capabilities.
3.  **Rollout:**
    *   Merge changes to the main branch.
    *   Deploy the updated MCP server.
    *   Announce the new tools and their parameters to users.

### **Risks and Mitigations**

*   **Risk:** `ParallelEngine` performance bottlenecks with I/O-heavy file crawling.
    *   **Mitigation:** The engine is already designed for this, but ensure the `max_concurrency` in `OptimizedConfig` is tuned appropriately. Use `asyncio` file operations where possible.
*   **Risk:** Inconsistent output or logging formats compared to the CLI.
    *   **Mitigation:** Heavily reference `cli/crawl.py` during development. The use of shared components (`OptimizedConfig`, `OutputManager`) is the primary mitigation.
*   **Risk:** Unhandled exceptions in the crawl strategies crash the tool.
    *   **Mitigation:** Implement comprehensive `try...except` blocks within the `ParallelEngine`'s worker tasks and within the strategies themselves. Report errors via the summary, not by crashing.
*   **Risk:** Tight coupling with `RagService` makes tools less modular.
    *   **Mitigation:** The `ingest_to_rag` flag provides a clean separation. The core crawling logic must complete and save its results *before* ingestion is attempted. The tool's primary return value should always be the crawl summary, not the RAG result.
