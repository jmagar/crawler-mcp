# Crawler MCP Environment Configuration
GITHUB_TOKEN=your_github_token_here
# =============================================================================
# SERVER CONFIGURATION
# =============================================================================
SERVER_HOST=0.0.0.0
SERVER_PORT=8010
DEBUG=true
PRODUCTION=false
LOG_LEVEL=INFO
LOG_FORMAT=console
LOG_FILE=logs/crawlerr.log
LOG_TO_FILE=true
PID_FILE=logs/crawlerr.pid

# =============================================================================
# SERVICE ENDPOINTS
# =============================================================================
# Qdrant Vector Database - Performance Optimized
QDRANT_URL=http://localhost:7000
QDRANT_API_KEY=
QDRANT_COLLECTION=crawlerr_documents
QDRANT_VECTOR_SIZE=1024
QDRANT_DISTANCE=cosine
QDRANT_TIMEOUT=10.0
QDRANT_RETRY_COUNT=3
QDRANT_CONNECTION_POOL_SIZE=16
QDRANT_BATCH_SIZE=256
QDRANT_PREFETCH_SIZE=1024

# HF Text Embeddings Inference (TEI) - RTX 4070 Optimized
TEI_URL=http://localhost:8080
TEI_MODEL=Qwen/Qwen3-Embedding-0.6B
TEI_MAX_CONCURRENT_REQUESTS=80
TEI_MAX_BATCH_TOKENS=163840
TEI_BATCH_SIZE=80
TEI_TIMEOUT=60.0

# Embedding Configuration
EMBEDDING_MAX_LENGTH=8192
EMBEDDING_DIMENSION=1024
EMBEDDING_NORMALIZE=true
EMBEDDING_MAX_RETRIES=2
EMBEDDING_WORKERS=4

# Chunking Configuration (Optimized for Qwen3-Embedding-0.6B)
CHUNK_SIZE=4096
CHUNK_OVERLAP=512
WORD_TO_TOKEN_RATIO=1.4

# Reranker Configuration
RERANKER_MODEL=tomaarsen/Qwen3-Reranker-0.6B-seq-cls
RERANKER_ENABLED=true
RERANKER_TOP_K=10
RERANKER_MAX_LENGTH=512
RERANKER_FALLBACK_TO_CUSTOM=true

# =============================================================================
# CRAWLING CONFIGURATION
# =============================================================================
# Basic Crawling Settings
CRAWL_HEADLESS=true
CRAWL_BROWSER=chromium
CRAWL_MAX_PAGES=1000
CRAWL_MAX_DEPTH=3
MAX_CONCURRENT_CRAWLS=10
CRAWLER_DELAY=0.05
CRAWLER_TIMEOUT=5.0
CRAWL_MIN_WORDS=50
CRAWL_REMOVE_OVERLAYS=true
CRAWL_EXTRACT_MEDIA=false
CRAWL_KNOWLEDGE_GRAPH=false
CRAWL_USER_AGENT=crawler-mcp/0.1.0 (+https://github.com/jmagar/crawler-mcp)

# Advanced Crawling Features (Crawl4AI 0.7.0+) - Performance Optimized
CRAWL_ADAPTIVE_MODE=true
CRAWL_CONFIDENCE_THRESHOLD=0.8
CRAWL_TOP_K_LINKS=5
CRAWL_URL_SEEDING=true
CRAWL_SCORE_THRESHOLD=0.4
CRAWL_VIRTUAL_SCROLL=true
CRAWL_SCROLL_COUNT=50
CRAWL_SCROLL_DELAY=50
CRAWL_VIRTUAL_SCROLL_BATCH_SIZE=10
CRAWL_MEMORY_THRESHOLD=70.0

# Performance Optimization
CRAWL_ENABLE_STREAMING=true
CRAWL_ENABLE_CACHING=true

# Resource blocking (performance)
CRAWL_BLOCK_IMAGES=true
CRAWL_BLOCK_STYLESHEETS=true
CRAWL_BLOCK_FONTS=true
CRAWL_BLOCK_MEDIA=true

# =============================================================================
# DEDUPLICATION CONFIGURATION
# =============================================================================
DEDUPLICATION_ENABLED=true
DEDUPLICATION_STRATEGY=content_hash
DELETE_ORPHANED_CHUNKS=true
PRESERVE_UNCHANGED_METADATA=true

# =============================================================================
# GPU ACCELERATION
# =============================================================================
GPU_ACCELERATION=true
CRAWL_GPU_ENABLED=true

# =============================================================================
# HIGH-PERFORMANCE CONFIGURATION (i7-13700k + RTX 4070)
# =============================================================================
# Browser Pool Configuration
BROWSER_POOL_SIZE=8

# File Processing Configuration
FILE_PROCESSING_THREADS=16

# Crawl Concurrency Configuration
CRAWL_CONCURRENCY=30

# Alternative Crawling Approach Settings
USE_ARUN_MANY_FOR_SITEMAPS=false
CRAWL_MAX_CONCURRENT_SESSIONS=20

# Crawl4AI Performance Optimizations
CRAWL_TEXT_MODE=true
CRAWL_LIGHT_MODE=true
USE_LXML_STRATEGY=true

# Content Caching Configuration
CONTENT_CACHE_SIZE_GB=8

# GPU Memory Configuration
GPU_MEMORY_FRACTION=0.95

# =============================================================================
# CORS & SECURITY
# =============================================================================
CORS_ORIGINS=*
CORS_CREDENTIALS=true
MAX_REQUEST_SIZE=10485760
REQUEST_TIMEOUT=30.0

# =============================================================================
# GITHUB WEBHOOK SERVER CONFIGURATION
# =============================================================================
# Required: GitHub webhook secret (set in GitHub organization settings)
GITHUB_WEBHOOK_SECRET=your_webhook_secret_here

# Webhook server port (different from main MCP server)
WEBHOOK_PORT=38080

# Repository filtering (* for all, or comma-separated list: "owner/repo1,owner/repo2")
REPOS_TO_TRACK=*

# Webhook processing configuration
WEBHOOK_SCRIPT_PATH=./scripts/extract_coderabbit_prompts.py
WEBHOOK_OUTPUT_DIR=./webhook_outputs
WEBHOOK_MAX_CONCURRENT_PROCESSES=5

# Event type filtering (true/false)
PROCESS_REVIEWS=true
PROCESS_REVIEW_COMMENTS=true
PROCESS_ISSUE_COMMENTS=true

# Bot patterns to extract content from (comma-separated)
BOT_PATTERNS=coderabbitai[bot],copilot-pull-request-reviewer[bot],Copilot

# =============================================================================
# PR CLI INTEGRATION (New!)
# =============================================================================
# Enable new PR CLI tool (true) or use legacy script (false)
WEBHOOK_USE_PR_CLI=true

# Output directory for PR CLI structured outputs
WEBHOOK_PR_OUTPUT_DIR=./output

# Generate backward-compatible markdown files alongside structured output
WEBHOOK_PRESERVE_MARKDOWN=false

# Advanced PR filters as JSON string (optional)
# Example: {"only_unresolved": true, "min_length": 50}
WEBHOOK_PR_FILTERS={}
