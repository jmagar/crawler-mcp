services:
  qdrant:
    container_name: crawler-qdrant
    image: qdrant/qdrant:v1.15.1
    ports:
      - "7000:6333"
      - "7001:6334"  # gRPC port
    volumes:
      - qdrant_data:/qdrant/storage
    environment:
      QDRANT__LOG_LEVEL: "INFO"
      QDRANT__SERVICE__HTTP_PORT: 6333
      QDRANT__SERVICE__GRPC_PORT: 6334
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:6333/health"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 40s
    restart: unless-stopped
    networks:
      - crawler-network

  text-embeddings-inference:
    image: ghcr.io/huggingface/text-embeddings-inference:1.8
    container_name: crawler-embeddings
    ports:
      - "8080:80"
    volumes:
      - tei_data:/data
    command:
      - --model-id
      - Qwen/Qwen3-Embedding-0.6B
      - --dtype
      - float16
      - --max-concurrent-requests
      - "80"
      - --max-batch-tokens
      - "163840"
      - --max-batch-requests
      - "80"
      - --max-client-batch-size
      - "128"
      - --pooling
      - "last-token"
      - --tokenization-workers
      - "8"
      - --auto-truncate
    environment:
      - NVIDIA_VISIBLE_DEVICES=0
      - CUDA_VISIBLE_DEVICES=0
      - PYTORCH_CUDA_ALLOC_CONF=max_split_size_mb:1024
      - OMP_NUM_THREADS=8
      - TOKENIZERS_PARALLELISM=true
      - CUDA_CACHE_DISABLE=0
      - RUST_LOG=text_embeddings_router=info
      - HF_HUB_CACHE=/data/cache
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:80/health"]
      interval: 30s
      timeout: 10s
      retries: 3
    restart: unless-stopped
    networks:
      - crawler-network

volumes:
  tei_data:
  qdrant_data:

networks:
  crawler-network:
    driver: bridge
